<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>OpenAI Lab 手册</title>

    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight {
  color: #faf6e4;
  background-color: #122b3b;
}
.highlight .gl {
  color: #dee5e7;
  background-color: #4e5d62;
}
.highlight .c, .highlight .cd, .highlight .cm, .highlight .c1, .highlight .cs {
  color: #6c8b9f;
  font-style: italic;
}
.highlight .cp {
  color: #b2fd6d;
  font-weight: bold;
  font-style: italic;
}
.highlight .err {
  color: #fefeec;
  background-color: #cc0000;
}
.highlight .gr {
  color: #fefeec;
  background-color: #cc0000;
}
.highlight .k, .highlight .kd, .highlight .kv {
  color: #f6dd62;
  font-weight: bold;
}
.highlight .o, .highlight .ow {
  color: #4df4ff;
}
.highlight .p, .highlight .pi {
  color: #4df4ff;
}
.highlight .gd {
  color: #cc0000;
}
.highlight .gi {
  color: #b2fd6d;
}
.highlight .ge {
  font-style: italic;
}
.highlight .gs {
  font-weight: bold;
}
.highlight .gt {
  color: #dee5e7;
  background-color: #4e5d62;
}
.highlight .kc {
  color: #f696db;
  font-weight: bold;
}
.highlight .kn {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kp {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kr {
  color: #ffb000;
  font-weight: bold;
}
.highlight .gh {
  color: #ffb000;
  font-weight: bold;
}
.highlight .gu {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kt {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .no {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nc {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nd {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nn {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .bp {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .ne {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nl {
  color: #ffb000;
  font-weight: bold;
}
.highlight .nt {
  color: #ffb000;
  font-weight: bold;
}
.highlight .m, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mb, .highlight .mx {
  color: #f696db;
  font-weight: bold;
}
.highlight .ld {
  color: #f696db;
  font-weight: bold;
}
.highlight .ss {
  color: #f696db;
  font-weight: bold;
}
.highlight .s, .highlight .sb, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .sr, .highlight .s1 {
  color: #fff0a6;
  font-weight: bold;
}
.highlight .se {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .sc {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .si {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .nb {
  font-weight: bold;
}
.highlight .ni {
  color: #999999;
  font-weight: bold;
}
.highlight .w {
  color: #BBBBBB;
}
.highlight .nf {
  color: #a8e1fe;
}
.highlight .py {
  color: #a8e1fe;
}
.highlight .na {
  color: #a8e1fe;
}
.highlight .nv, .highlight .vc, .highlight .vg, .highlight .vi {
  color: #a8e1fe;
  font-weight: bold;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="tocify-wrapper">
      <img src="images/logo.png" alt="Logo" />
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc">
      </div>
        <ul class="toc-footer">
            <li><a href='https://github.com/kengz/openai_lab'>OpenAI Lab Github</a></li>
            <li><a href='https://github.com/openai/gym'>OpenAI Gym Github</a></li>
            <li><a href='https://github.com/fchollet/keras'>Keras Github</a></li>
            <li><a href='https://youtu.be/qBhLoeijgtA'>RL Tutorial video part 1/2</a></li>
            <li><a href='https://youtu.be/wNSlZJGdodE'>RL Tutorial video part 2/2</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id='openai-lab-br-a-href-https-github-com-kengz-openai_lab-img-src-https-img-shields-io-github-release-kengz-openai_lab-svg-alt-github-release-a-a-href-https-circleci-com-gh-kengz-openai_lab-img-src-https-circleci-com-gh-kengz-openai_lab-svg-style-shield-alt-circleci-a-a-href-https-www-codacy-com-app-kengzwl-openai_lab-utm_source-github-com-amp-amp-utm_medium-referral-amp-amp-utm_content-kengz-openai_lab-amp-amp-utm_campaign-badge_grade-img-src-https-api-codacy-com-project-badge-grade-9e55f845b10b4b51b213620bfb98e4b3-alt-codacy-badge-a-a-href-https-www-codacy-com-app-kengzwl-openai_lab-utm_source-github-com-amp-utm_medium-referral-amp-utm_content-kengz-openai_lab-amp-utm_campaign-badge_coverage-img-src-https-api-codacy-com-project-badge-coverage-9e55f845b10b4b51b213620bfb98e4b3-alt-codacy-badge-a-a-href-https-github-com-kengz-openai_lab-img-src-https-img-shields-io-github-stars-kengz-openai_lab-svg-style-social-amp-label-star-alt-github-stars-a-a-href-https-github-com-kengz-openai_lab-img-src-https-img-shields-io-github-forks-kengz-openai_lab-svg-style-social-amp-label-fork-alt-github-forks-a'>OpenAI Lab </br> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/release/kengz/openai_lab.svg" alt="GitHub release" /></a> <a href="https://circleci.com/gh/kengz/openai_lab"><img src="https://circleci.com/gh/kengz/openai_lab.svg?style=shield" alt="CircleCI" /></a> <a href="https://www.codacy.com/app/kengzwl/openai_lab?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=kengz/openai_lab&amp;amp;utm_campaign=Badge_Grade"><img src="https://api.codacy.com/project/badge/Grade/9e55f845b10b4b51b213620bfb98e4b3" alt="Codacy Badge" /></a> <a href="https://www.codacy.com/app/kengzwl/openai_lab?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=kengz/openai_lab&amp;utm_campaign=Badge_Coverage"><img src="https://api.codacy.com/project/badge/Coverage/9e55f845b10b4b51b213620bfb98e4b3" alt="Codacy Badge" /></a> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/stars/kengz/openai_lab.svg?style=social&amp;label=Star" alt="GitHub stars" /></a> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/forks/kengz/openai_lab.svg?style=social&amp;label=Fork" alt="GitHub forks" /></a></h1>
<p><em>For English doc, refer to the original <a href="http://kengz.me/openai_lab/">OpenAI Lab doc.</a></em></p>

<p><em>使用 OpenAI Gym，Tensorflow 和 Keras 的加强学习 (Reinforcement Learning) 实验框架。</em></p>

<p><em>OpenAI Lab</em><em>让人们把加强学习 (Reinforcement Learning / RL) 像科学一样来 _理论，实验</em>。 它为 <a href="https://gym.openai.com/">OpenAI Gym</a> 和 <a href="https://keras.io/">Keras</a> 提供了一个简单的界面， 并具有自动化的实验和评估框架。</p>
<h3 id=''>特征</h3>
<ol>
<li>使用 OpenAI Gym，Tensorflow，Keras 的<strong>统一RL环境和界面</strong>，因此您可以专注于研究算法。</li>
<li><strong><a href="#agents-matrix">核心RL算法</a>, 和可重复使用的模块化组件</strong> 来研究深度RL算法.</li>
<li><strong><a href="#experiments">实验框架</a></strong> 能够运行数百次超参数优化试验，具有用于测试新的RL算法的日志，绘图和分析。 实验设置存储在标准化的JSON中，可用于重现性和比较。</li>
<li><strong><a href="#analysis">实验的自动分析</a></strong> 用于评估RL代理和环境，并帮助选择最佳解决方案。</li>
<li><strong><a href="#fitness-matrix">Fitness Matrix</a></strong>, 一个RL算法最佳分数表 v.s. 环境; 有助于研究。</li>
</ol>

<p>使用OpenAI Lab，我们可以专注于研究强化学习的基本要素，如算法，策略，内存和参数调优。 它允许我们使用现有组件与研究思想的实现有效地构建代理。 然后，我们可以通过运行实验系统地测试研究假设。</p>

<p><em>详细了解 Lab 在动机 <a href="#motivations">Motivations</a> 中的研究问题。 最终，Lab 是进行强化学习的广义框架，与 OpenAI Gym 和 Keras 无关系。 例如。 基于Pytorch的实现在路线图上。</em></p>
<h3 id='-2'>实现算法</h3>
<p>实现/计划的核心RL算法的列表。</p>

<p>要看看他们对 OpenAI Gym 环境的分数，请去 <strong><a href="#fitness-matrix">Fitness Matrix</a></strong>.</p>

<table><thead>
<tr>
<th style="text-align: left">algorithm</th>
<th style="text-align: left">implementation</th>
<th style="text-align: left">eval score (pending)</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1312.5602">DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/dqn.py">DQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.06461">Double DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/double_dqn.py">DoubleDQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.06581">Dueling DQN</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/deep_sarsa.py">DeepSarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Off-Policy Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/offpol_sarsa.py">OffPolicySarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.05952">PER (Prioritized Experience Replay)</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/memory/prioritized_exp_replay.py">PrioritizedExperienceReplay</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">CEM (Cross Entropy Method)</a></td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://incompleteideas.net/sutton/williams-92.pdf">REINFORCE</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf">DPG (Deterministic Policy Gradient) off-policy actor-critic</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/actor_critic.py">ActorCritic</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.02971">DDPG (Deep-DPG) actor-critic with target networks</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/ddpg.py">DDPG</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/pdf/1602.01783.pdf">A3C (asynchronous advantage actor-critic)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Dyna</td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1502.05477">TRPO</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Q*(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Retrace(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control (NEC)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1612.00796">EWC (Elastic Weight Consolidation)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h3 id='lab'>运行 Lab</h3>
<p>接下来，看安装 <a href="#installation">Installation</a> 并跳转到快速入门 <a href="#quickstart">Quickstart</a>.</p>

<div style="max-width: 100%"><img alt="Timelapse of OpenAI Lab" src="./images/lab_demo_dqn.gif" /></div>

<p><em>Timelapse of OpenAI Lab, solving CartPole-v0.</em></p>

          <h1 id='a-name-installation-a'><a name="installation"></a>安装</h1>
<p><strong>1.</strong> <strong>克隆资源库</strong></p>

<p><code>git clone https://github.com/kengz/openai_lab.git</code></p>

<p><em>如果您计划提交代码，请将此资源库替换为克隆代码。</em></p>

<p><strong>2.</strong> <strong>安装依赖关系</strong></p>

<p>运行以下命令进行安装：</p>

<ul>
<li>系统依赖于您的操作系统</li>
<li>项目依赖关系</li>
</ul>

<p><em>要在远程服务器上快速重复设置，而不是使用这些命令，请运行等效的安装脚本： <code>./bin/setup</code></em></p>
<pre class="highlight shell tab-shell"><code><span class="c"># cd into project directory</span>
<span class="nb">cd </span>openai_lab/

<span class="c">### MacOS System Dependencies</span>
<span class="c"># Homebrew</span>
ruby -e <span class="s2">"</span><span class="k">$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install<span class="k">)</span><span class="s2">"</span>
<span class="c"># OpenAI Gym dependencies</span>
brew install cmake boost boost-python sdl2 swig wget
<span class="c"># noti</span>
<span class="o">(</span>curl -L https://github.com/variadico/noti/releases/download/v2.5.0/noti2.5.0.darwin-amd64.tar.gz | tar -xz<span class="o">)</span>; sudo mv noti /usr/local/bin/
<span class="c"># Node &gt;= v7.0</span>
brew install node
<span class="c"># Python &gt;= v3.0</span>
brew install python3

<span class="c">### Linux Ubuntu System Dependencies</span>
sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test <span class="o">&amp;&amp;</span> sudo apt-get update
sudo apt-get install -y gcc-4.9 g++-4.9 libhdf5-dev libopenblas-dev git python3-tk tk-dev
<span class="c"># OpenAI Gym dependencies</span>
sudo apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig
<span class="c"># noti</span>
<span class="o">(</span>curl -L https://github.com/variadico/noti/releases/download/v2.5.0/noti2.5.0.linux-amd64.tar.gz | tar -xz<span class="o">)</span>; sudo mv noti /usr/local/bin/
<span class="c"># Node &gt;= v7.0</span>
<span class="o">(</span>curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -<span class="o">)</span>; sudo apt-get install -y nodejs
<span class="c"># Python &gt;= v3.0</span>
sudo apt-get -y install python3-dev python3-pip python3-setuptools


<span class="c">### Project Dependencies</span>
./bin/copy-config
npm install; sudo npm i -g grunt-cli
<span class="c"># option 1: pip (ensure it is python3)</span>
pip3 install -r requirements.txt
<span class="c"># option 2: virtualenv</span>
virtualenv .env
<span class="nb">source</span> .env/bin/activate
pip3 install -r requirements.txt
<span class="c"># option 3: conda</span>
conda env create -f environment.yml
<span class="nb">source </span>activate openai_lab
</code></pre>
<p><strong>3.</strong> <strong>设置配置文件</strong></p>

<p>运行 <code>./bin/copy-config</code>。 这将从模板创建配置文件，需要 Lab 使用 <a href="#usage">usage</a>:</p>

<ul>
<li><code>config/default.json</code> 用于本地开发，当 <code>grunt</code> 在没有 <code>-prod</code> 生产标志的情况下运行时使用。</li>
<li><code>config/production.json</code> 当 <code>grunt -prod</code> 与生产标志<code>-prod</code>一起运行时，用于生产 Lab 运行。</li>
</ul>
<h2 id='a-name-quickstart-a'><a name="quickstart"></a>快速入门</h2>
<p>实验室 Lab 带有最佳解决方案的实验。在下面运行你的第一个。</p>
<h3 id='single-trial'>Single Trial</h3>
<p>使用 Lab 命令运行单一最佳试验：<code>grunt -best</code></p>

<p>或者，上面引用的简单python命令是：<code>python3 main.py -e quickstart_dqn</code></p>

<aside class="notice">
记住在使用普通的python命令时激活 virtualenv / conda。
</aside>

<p>然后检查您的 <code>./data/</code> 文件夹中的图形和数据文件。</p>

<p>建议使用grunt命令，因为更容易安排和运行多个实验。 它来自 <code>config/default.json</code> ，现在应该有 <code>quickstart_dqn</code>; 更多可以添加。</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"data_sync_destination"</span><span class="p">:</span><span class="w"> </span><span class="s2">"~/Dropbox/openai_lab/data"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_DEST"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#rl-monitor"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_TOK"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GET_SLACK_BOT_TOKEN_FROM_https://my.slack.com/services/new/bot"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"experiments"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"quickstart_dqn"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>这个试验是 <code>DQN</code> 解决 <code>Cartpole-v0</code> 的最好的找到的解决方案代理 <a href="https://github.com/kengz/openai_lab/pull/73">the best found solution agent</a>。 你应该看到实验室运行如下：</p>

<p><img src="images/lab_demo_dqn.gif" title="Timelapse of OpenAI Lab" alt="Lab demo dqn" /></p>
<h3 id='experiment-with-multiple-trials'>Experiment with Multiple Trials</h3>
<p>下一步是运行一个小型实验，搜索最佳的试用解决方案。</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"quickstart_dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="err">...</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>这位于 <code>rl/spec/classic_experiment_specs.json</code> 的 <code>quickstart_dqn</code> 下。 实验研究了不同学习率 <code>lr</code> 和 DQN 神经网络架构 <code>hidden_layers</code> 的影响。 如果您愿意，请更改 <code>param_range</code> 以尝试更多的值。</p>

<p>然后，运行：<code>grunt</code></p>

<p>或者，简单的python命令是：<code>python3 main.py -bp -e dqn</code></p>

<p>然后检查您的 <code>./data/</code> 文件夹中的图形和数据文件。</p>

<p>实验需要约15分钟（取决于您的机器）。 它将从试验中产生实验数据。参考分析 <a href="#analysis">Analysis</a> 如何解释它们。</p>
<h3 id=''>下一个</h3>
<p>我们推荐：</p>

<ul>
<li>继续阅读下面的可选安装步骤。</li>
<li>解决方案 <a href="#solutions">Solutions</a> ，查看一些现有的解决方案来启动您的代理商，以及找到环境/高分以打败。</li>
<li>代理 <a href="#agents">Agents</a> 关于如何从现有组件创建代理，然后添加您自己的。</li>
<li>用法 <a href="#usage">Usage</a> 继续阅读文档。</li>
</ul>
<h2 id='-2'>更新实验室</h2>
<p>检查实验室的最新发布版本 <a href="https://github.com/kengz/openai_lab/releases">release versions here</a>.</p>

<ul>
<li>如果您直接克隆Lab，请用<code>git pull</code>更新</li>
<li>如果你分叉，设置远程 <a href="https://help.github.com/articles/configuring-a-remote-for-a-fork/">setup a remote</a> 和 <a href="https://help.github.com/articles/syncing-a-fork/">update fork</a></li>
</ul>
<pre class="highlight shell tab-shell"><code><span class="c"># update direct clone</span>
git pull

<span class="c"># update fork</span>
git fetch upstream
git merge upstream/master
</code></pre><h2 id='-3'>设置数据自动同步</h2>
<p>在远程服务器上运行实验室时，我们发现数据文件同步非常有用。 这使我们可以在我们的Dropbox应用程序，计算机或智能手机上实时查看实验图和数据。</p>

<p>对于自动同步实验室 <code>data/</code>，我们使用<a href="http://gruntjs.com/">Grunt</a>文件监视器将数据文件自动复制到Dropbox。 在你的收件箱中，设置一个共享文件夹 <code>~/Dropbox/openai_lab/data</code> 并同步到桌面。</p>

<p>在 <code>config / {default.json，production.json}</code> 中设置配置密钥 <code>data_sync_destination</code>。</p>

<aside class="notice">
这一步是可选的; 仅在运行生产模式时才需要。
</aside>
<h2 id='-4'>设置自动通知</h2>
<p>实验需要一段时间才能运行，我们发现完成后会自动通知实验。 我们使用 <a href="https://github.com/variadico/noti">noti</a>，它也随 <code>bin/setup</code> 一起安装。</p>

<p>设置一个Slack，创建一个新的通道 <code>#rl_monitor</code>，并获得一个 <a href="https://my.slack.com/services/new/bot">Slack bot token</a>.</p>

<p>在 <code>config/{default.json, production.json}</code>中设置配置键 <code>NOTI_SLACK_DEST</code>, <code>NOTI_SLACK_TOK</code>。</p>

<aside class="notice">
这一步是可选的; 运行生产模式时很有用。
</aside>

<p><img src="images/noti.png" title="Notifications from the lab running on our remote server beast" alt="Noti" />
<em>Notifications from the lab running on our remote server beast.</em></p>
<h2 id='-5'>硬件</h2>
<p>为了设置自己的硬件，特别是使用GPU，谷歌搜索将帮助我们更多的可能。 另外，设置通常是不平凡的，因为有这么多移动部件。 以下是推荐的参考文献：</p>

<ul>
<li><a href="https://pcpartpicker.com/list/xdbWBP">A ~$1000 PC build</a> (more expensive now ~$1200; buy your parts during Black Friday/sales.)</li>
<li><a href="https://www.tensorflow.org/install/install_linux">The official TensorFlow installation guide, with GPU setup info</a></li>
<li><a href="http://christopher5106.github.io/nvidia/2016/12/30/commands-nvidia-install-ubuntu-16-04.html">Getting CUDA 8 to Work With openAI Gym on AWS and Compiling Tensorflow for CUDA 8 Compatibility</a></li>
<li><a href="https://github.com/openai/gym/issues/366">Major OpenAI issue with SSH with xvfb failing with NVIDIA Driver due to opengl files</a></li>
<li><a href="http://askubuntu.com/questions/149206/how-to-install-nvidia-run">NVIDIA cannot install due to X server running</a></li>
<li><a href="http://askubuntu.com/questions/759641/cant-get-nvidia-drivers-working-with-16-04-logs-out-right-after-login">When login fails on Ubuntu after Nvidia installation</a></li>
</ul>

          <h1 id='a-name-usage-a-usage'><a name="usage"></a>Usage</h1>
<p><em>To understand the Lab&#39;s <a href="#structure">Framework and Demo, skip to the next section.</a></em></p>

<p>The general flow for running a production lab is:</p>

<ol>
<li>Specify experiment specs in <code>rl/spec/*_experiment_specs.json</code>, e.g. <code>&quot;dqn&quot;, &quot;lunar_dqn&quot;</code></li>
<li>Specify the names of the experiments to run in <code>config/production.json</code></li>
<li>Run the lab, e.g. <code>grunt -prod -resume</code></li>
</ol>

<p>Grunt will read off the JSON file in <code>config/</code>, which looks like:</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"data_sync_destination"</span><span class="p">:</span><span class="w"> </span><span class="s2">"~/Dropbox/openai_lab/data"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_DEST"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#rl-monitor"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_TOK"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GET_SLACK_BOT_TOKEN_FROM_https://my.slack.com/services/new/bot"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"experiments"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"dqn"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"lunar_dqn"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre><h2 id='commands'>Commands</h2>
<p>We use <a href="http://gruntjs.com/">Grunt</a> to run the lab - set up experiments, pause/resume lab, run analyses, sync data, notify on completion. Internally <code>grunt</code> runs the <code>python</code> command (harder to use), logged to stdout as <code>&gt;&gt; Composed command: ... python3 main.py -bp -t 5 -e dev_dqn | tee ./data/terminal.log;</code></p>

<p>The useful grunt commands are:</p>
<pre class="highlight shell tab-shell"><code><span class="c"># when developing experiments specified in default.json</span>
grunt
<span class="c"># developing, run single best trial without param selection</span>
grunt -best

<span class="c"># run real lab experiments specified in production.json</span>
grunt -prod
<span class="c"># run lab over ssh on remote server</span>
grunt -prod -remote
<span class="c"># resume lab (previously incomplete experiments)</span>
grunt -prod -remote -resume


<span class="c"># clear data/ folder and cache files</span>
grunt clear
</code></pre>
<p>See below for the full <a href="#grunt-cmd">Grunt Command Reference</a> or the <a href="#python-cmd">Python Command Reference</a>.</p>

<p><strong>development</strong> mode:</p>

<ul>
<li>All grunt commands default to this mode</li>
<li>specify your dev experiment in <code>config/default.json</code></li>
<li>use only when developing your new algorithms</li>
<li>the file-sync is in mock mode (emulated log without real file copying)</li>
<li>no auto-notification</li>
</ul>

<p><strong>production</strong> mode:</p>

<ul>
<li>append the flag <code>-prod</code> to your <code>grunt</code> command</li>
<li>specify your full experiments in <code>config/production.json</code></li>
<li>use when running experiments for real</li>
<li>the file-sync is real</li>
<li>has auto-notification to Slack channel</li>
</ul>
<h2 id='run-remotely'>Run Remotely</h2>
<p>If you&#39;re using a remote server, run the commands inside a <code>screen</code>. That is, log in via ssh, start a screen, run, then detach screen.</p>
<pre class="highlight shell tab-shell"><code><span class="c"># enter the screen with the name "lab"</span>
screen -S lab
<span class="c"># run real lab over ssh, in resume mode</span>
grunt -prod -remote -resume
<span class="c"># use Cmd+A+D to detach from screen, then Cmd+D to disconnect ssh</span>

<span class="c"># to resume screen next time</span>
screen -r lab
<span class="c"># use Cmd+D to terminate screen when lab ends</span>
</code></pre>
<p>Since a remote server is away, you should check the system status occasionally to ensure no overrunning processes (memory growth, large processes, overheating). Use <a href="https://github.com/nicolargo/glances"><code>glances</code></a> (already installed in <code>bin/setup</code>) to monitor your expensive machines.</p>

<aside class="notice">
To monitor your system (CPU, RAM, GPU), run <code>glances</code>
</aside>

<p><img src="images/glances.png" title="Glances to monitor your system" alt="Glances" />
<em>Glances on remote server beast.</em></p>
<h2 id='resume-lab'>Resume Lab</h2>
<p>Experiments take a long time to complete, and if your process gets terminated, resuming the lab is trivial with a <code>-resume</code> flag: <code>grunt -prod -remote -resume</code>. This will use the <code>config/history.json</code>:</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn-2017_03_19_004714"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>The <code>config/history.json</code> is created in the last run that maps <code>experiment_name</code>s to <code>experiment_id</code>s, and resume any incomplete experiments based on that <code>experiment_id</code>. You can manually tweak the file to set the resume target of course.</p>
<h2 id='a-name-grunt-cmd-a-grunt-command-reference'><a name="grunt-cmd"></a>Grunt Command Reference</h2>
<p>By default the <code>grunt</code> command (no task or flag) runs the lab in <code>development</code> mode using <code>config/default.json</code>.</p>

<p>The basic grunt command pattern is</p>
<pre class="highlight shell tab-shell"><code>grunt &lt;task&gt; -&lt;flag&gt;

<span class="c"># again, the useful grunt commands are:</span>

<span class="c"># when developing experiments specified in default.json</span>
grunt
<span class="c"># developing, run single best trial without param selection</span>
grunt -best

<span class="c"># run real lab experiments specified in production.json</span>
grunt -prod
<span class="c"># run lab over ssh on remote server</span>
grunt -prod -remote
<span class="c"># resume lab (previously incomplete experiments)</span>
grunt -prod -remote -resume

<span class="c"># clear data/ folder and cache files</span>
grunt clear
</code></pre>
<p>The <code>&lt;task&gt;</code>s are:</p>

<ul>
<li><em>(default empty)</em>: run the lab</li>
<li><code>clear</code>: clear the <code>data/</code> folder and cache files. <strong>Be careful</strong> and make sure your data is already copied to the sync location</li>
</ul>

<p>The <code>&lt;flag&gt;</code>s are:</p>

<ul>
<li><code>-prod</code>: production mode, use <code>config/production.json</code></li>
<li><code>-resume</code>: resume incomplete experiments from <code>config/history.json</code></li>
<li><code>-remote</code>: when running over SSH, supplies this to use a fake display</li>
<li><code>-best</code>: run the finalized experiments with gym rendering and live plotting; without param selection. This uses the default <code>param</code> in <code>experiment_specs.json</code> that shall be updated to the best found.</li>
<li><code>-debug</code>: verbose debug logging. This is for lab-level development only.</li>
<li><code>-quiet</code>: mute all python logging in grunt. This is for lab-level development only.</li>
</ul>
<h2 id='a-name-python-cmd-a-python-command-reference'><a name="python-cmd"></a>Python Command Reference</h2>
<p>The Python command is invoked inside <code>Gruntfile.js</code> under the <code>composeCommand</code> function. Change it if you need to.</p>

<aside class="notice">
Remember to activate virtualenv/conda when using plain python commands.
</aside>

<p>The basic python command pattern is:</p>
<pre class="highlight shell tab-shell"><code>python3 main.py -&lt;flag&gt;

<span class="c"># most common example, with piping of terminal log</span>
python3 main.py -bp -t 5 -e dqn | tee -a ./data/terminal.log;
</code></pre>
<p>The python command <flag>s are:</p>

<ul>
<li><code>-b</code>: blind mode, do not render graphics. Default: <code>False</code></li>
<li><code>-d</code>: log debug info. Default: <code>False</code></li>
<li><code>-e &lt;experiment&gt;</code>: specify which inside the <code>rl/spec/*_experiment_spec.json</code> to run. Default: <code>-e dev_dqn</code>. Can be a <code>experiment_name, experiment_id</code>.</li>
<li><code>-p</code>: run param selection. Default: <code>False</code></li>
<li><code>-q</code>: quiet mode, log warning only. Default: <code>False</code></li>
<li><code>-t &lt;times&gt;</code>: the number of sessions to run per trial. Default: <code>1</code></li>
<li><code>-x &lt;max_episodes&gt;</code>: Manually specifiy max number of episodes per trial. Default: <code>-1</code> and program defaults to value in <code>rl/spec/problems.json</code></li>
</ul>

          <h1 id='a-name-experiments-a-experiments'><a name="experiments"></a>Experiments</h1>
<p>The experimental framework design and terminology should be familiar, since they&#39;re borrowed from experimental science. The Lab runs experiments and produces data for <a href="#analysis">analysis</a>.</p>
<h2 id='definition'>Definition</h2>
<p>An <strong>experiment</strong> runs separate <strong>trials</strong> by varying parameters. Each <strong>trial</strong> runs multiple <strong>sessions</strong> for averaging the results.</p>

<p>An experiment consists of:</p>

<ul>
<li>an <strong>environment</strong> (problem) from <a href="https://gym.openai.com/envs">OpenAI Gym</a></li>
<li>an <strong>agent</strong> to solve the environment.</li>
</ul>

<aside class="notice">
An experiment runs the variations of agent by changing its parameters (experiment variables) while holding others constants (control), and measure the fitness_score (outcome) to solve the environment.
</aside>
<h2 id='specification'>Specification</h2>
<p>An experiment is specified by an <code>experiment_spec</code> in <code>rl/spec/*_experiment_specs.json</code>.</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>It consists of:</p>

<ul>
<li><code>experiment_name</code>: the key of the JSON. e.g. <code>dqn</code></li>
<li><code>problem</code>: name of the environment. e.g. <code>CartPole-v0</code></li>
<li><strong>agent</strong>: and its components in <code>rl/</code>, specified by the class name

<ul>
<li><code>Agent</code> (Learning algorithm): decision function for learning from experiences gained by acting in an environment (eg Q-Learning, Sarsa). This is also the main class for agents. All other components of an agent are contained within this class.</li>
<li><code>Policy</code>: decision function for acting in an environment. Controls exploration vs. exploitation trade off(e.g. epsilon greedy, boltzmann)</li>
<li><code>Memory</code>: for storing experiences gained by acting in an environment. Controls how experiences are sampled for an agent to learn from. (e.g. random uniform with no forgetting, prioritized sampling with forgetting)</li>
<li><code>Optimizer</code>: controls how to optimize the function approximators contained within the agent (e.g. Stochatic Gradient Descent, Adam) </li>
<li><code>HyperOptimizer</code>: hyperparameter optimization algorithms used to vary the agent parameters and run trials with them (e.g grid search, random search)</li>
<li><code>Preprocessor</code>: controls the transformations made to state representaions before being passed as inputs to the policy and learning algorithm. (e.g. no preprocessing, concatenating current and previous state). Useful for Atari.</li>
</ul></li>
<li><code>param</code>: the default parameter values used (control variables)</li>
<li><code>param_range</code>: the hyperparameter space ranges to search through by <code>HyperOptimimzer</code> (experiment variables).</li>
</ul>
<h2 id='a-name-structure-a-structure'><a name="structure"></a>Structure</h2>
<p>How <code>experiments &gt; trials &gt; sessions</code> are organized and ran.</p>

<p>When the Lab runs an <strong>experiment</strong> with <code>experiment_name</code> (e.g. <code>dqn</code>):</p>

<ul>
<li>it creates a timestamped <code>experiment_id</code> (<code>dqn-2017_03_19_004714</code>)</li>
<li>the <strong>experiment</strong> runs multiple trials over the hyperparameter space

<ul>
<li>the trials are ordered for resumability (in case machine dies)</li>
<li>each trial has <code>trial_id</code> (<code>dqn-2017_03_19_004714_t0</code>), tied to a unique set of param values</li>
<li>a <strong>trial</strong> runs multiple sessions

<ul>
<li>each <strong>session</strong> has <code>session_id</code> (<code>dqn-2017_03_19_004714_t0_s0</code>)</li>
<li>a session runs the environment-agent, produces graphs and <code>session_data</code> i.e. <code>sys_vars</code></li>
<li>the session saves its graph to <code>&lt;session_id&gt;.png</code></li>
<li>the session returns <code>sys_vars</code> to its trial</li>
</ul></li>
<li>the trial gathers all the <code>sys_vars</code>, run some averaging analytics, then compose all that into <code>trial_data</code></li>
<li>the trial returns the <code>trial_data</code> and saves it to <code>&lt;trial_id&gt;.json</code></li>
</ul></li>
<li>the experiment composes all <code>trial_data</code> into a <code>experiment_data</code></li>
<li>it runs analytics to produce graphs <code>&lt;experiment_id&gt;_analysis.png, &lt;experiment_id&gt;_correlation.png</code></li>
<li>it compute the <code>fitness_score</code> for each trial, rank them by best-first, then save the data grid to <code>&lt;experiment_id&gt;_analysis_data.csv</code></li>
<li>experiment ends</li>
</ul>
<h2 id='a-name-demo-a-lab-demo'><a name="demo"></a>Lab Demo</h2>
<p>Given the framework explained above, here&#39;s a quick demo.</p>

<p>Suppose we aim to solve the CartPole-v0 problem with the plain DQN agent.  Suppose again for this experiment, we implement a new agent component, namely a <code>Boltzmann</code> policy, and try to find the best parameter sets for this new agent.</p>
<h3 id='specify-experiment'>Specify Experiment</h3>
<p>The example below is fully specified in <code>rl/spec/classic_experiment_specs.json</code> under <code>dqn</code>:</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>Specifically of interests, we have specified the variables:</p>

<ul>
<li><em>experiment_name</em>: <code>dqn</code></li>
<li><em>problem</em>: <a href="https://gym.openai.com/envs/CartPole-v0">CartPole-v0</a></li>
<li><em>variable agent component</em>: <code>Boltzmann</code> policy</li>
<li><em>control agent variables</em>:

<ul>
<li><code>DQN</code> agent</li>
<li><code>LinearMemoryWithForgetting</code></li>
<li><code>AdamOptimizer</code></li>
<li><code>NoPreProcessor</code></li>
</ul></li>
<li><em>hyperparameter space</em>: the <code>&quot;param_range&quot;</code> JSON</li>
<li><em>hyperparameter optimizer</em>: <code>GridSearch</code></li>
</ul>

<p>Given <code>GridSearch HyperOptimizer</code>, this <strong>experiment</strong> will try all the discrete combinations of the <code>param_range</code>, which makes for <code>4x4x5=80</code> trials. Each <strong>trial</strong> will run a max of 5 <strong>sessions</strong> (terminate on 2 if fail to solve). Overall, this experiments will run at most <code>80 x 5 = 400</code> sessions, then produce <code>experiment_data</code> and the analytics.</p>
<h3 id='lab-workflow'>Lab Workflow</h3>
<p>The example workflow to setup this experiment is as follow:</p>

<ol>
<li>Add the new theorized component <code>Boltzmann</code> in <code>rl/policy/boltzmann.py</code></li>
<li>Specify <code>dqn</code> experiment spec in <code>rl/spec/classic_experiment_spec.json</code> to include this new variable, reuse the other existing RL components, and specify the param range.</li>
<li>Add this experiment to the lab queue in <code>config/production.json</code></li>
<li>Run experiment with <code>grunt -prod</code></li>
<li>Analyze the graphs and data</li>
</ol>

<p>Now that you can produce the experiment data and graphs, see how to <a href="#analysis">analyze them</a>.</p>

          <h1 id='a-name-analysis-a-analysis'><a name="analysis"></a>Analysis</h1>
<p>Once the Lab is running experiments, it will produce data. This section details how to analyze and understand the data, before we can contribute to the <a href="#solutions">Solutions</a>.</p>

<p>An experiment produces 3 types of data files in the folder <code>data/&lt;experiment_id&gt;/</code>:</p>

<ul>
<li><strong>session plots</strong>: <code>&lt;session_id&gt;.png</code></li>
<li><strong>trial_data</strong>: <code>&lt;trial_id&gt;.json</code></li>
<li><strong>experiment data</strong>:

<ul>
<li><code>&lt;experiment_id&gt;_analysis_data.csv</code></li>
<li><code>&lt;experiment_id&gt;_analysis.png</code></li>
<li><code>&lt;experiment_id&gt;_analysis_correlation.png</code></li>
</ul></li>
</ul>

<aside class="notice">
Refer to <a href="#structure">Experiments > Structure</a> to see how the files are produced.
</aside>

<p>We will illustrate with an example experiment from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn solution PR</a>.</p>
<h2 id='session-graphs'>Session Graphs</h2>
<blockquote>
<p><img alt="The best session graph" src="https://cloud.githubusercontent.com/assets/8209263/24180935/404370ea-0e8e-11e7-8f20-f8691ee03e7b.png" />
<em>The best session graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. From the session graph we can see that the agent starts learning the CartPole-v0 task at around episode 15, then solves it before episode 20. Over time the loss decreases, the solution becomes stabler, and the mean rewards increases until the session is solved reliably.</em></p>
</blockquote>

<p>When an experiment is running, the lab will plot the session graphs live, one for each session.</p>
<h3 id='how-to-read'>How to read</h3>
<p>A session graph has 3 subplots:</p>

<ol>
<li><p><strong>total rewards and exploration rate vs episode</strong>: directly records the (blue) total rewards attained at the end of each episode, in relation to the (red) exploration rate (<code>epsilon</code>, <code>tau</code>, etc. depending on the policy).</p>

<p>The 2 lines usually show negative correlation - when the exploration rate drops, the total rewards should rise. When a solution becomes stable, the blue line should stay around its max.</p></li>
<li><p><strong>mean rewards over the last 100 episodes vs episode</strong>: measures the 100-episode mean of the total rewards from above.</p>

<p>Defined by OpenAI, this metric is usually how a solution is identified - when it hits a target solution score, which would mean that the solution is sufficiently <em>strong</em> and <em>stable</em>.</p></li>
<li><p><strong>loss vs time</strong>: measures the loss of the agent&#39;s neural net. This graph is all the losses  concatenated over time, over all episodes.</p>

<p>There is no specific unit for the loss as it depends on what loss function is used in the NN architecture (typically <code>mean_squared_error</code>). As the NN starts getting more accurate, the loss should decrease.</p></li>
</ol>

<aside class="notice">
When developing a new algorithm, use the session graph to immediately see how the agent is performing without needing to wait for the entire session to complete.
</aside>
<h2 id='analysis-graph'>Analysis Graph</h2>
<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582680/bc9fd9ae-1702-11e7-881b-38f7f379d6ff.png" title="Analysis graph" alt="Bc9fd9ae 1702 11e7 881b 38f7f379d6ff" />
<em>The analysis graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. There&#39;re numerous dark points with solved_ratio 1, which is expected since CartPole-v0 is the simplest environment. There are clear trends cross the x-values - gamma=0.95 is unstable; 2-hidden-layer NN is unsuitable for the problem, but wider 1-hidden-layer is good; learning rate lr=0.001 is stabler, but lr=0.02 is a good balance between stability and fitness_score.</em></p>
</blockquote>

<p>The <strong>analysis graph</strong> is the primary graph used to judge the overall experiment - how all the trials perform. It is a pair-plot of the <em>measurement metrics on the y-axis</em>, and the <em>experiment variables on the x-axis</em>.</p>
<h3 id='the-y-axis-measurement-metrics'>The y-axis measurement metrics</h3>
<ol>
<li><p><code>fitness_score</code>: the final evaluation metric the Lab uses to select a fit agent (an agent with the fit parameter set for that class of Agent). The design and purpose of it is more involved - see <a href="#metrics">metrics</a> for more.</p></li>
<li><p><code>mean_rewards_stats_max</code>: the max of all the <code>mean_rewards</code> over all the sessions of a trial. Measures the max solution power of a trial.</p></li>
<li><p><code>max_total_rewards_stats_mean</code>: the statistical mean of all the <code>max_total_rewards</code> over all the sessions of a trial. Measures the agent&#39;s average peak performance.</p></li>
<li><p><code>epi_stats_min</code>: the min of the termination episode of a session, i.e. the fastest solved session of a trial. The lower the better, as it would imply that the agent can solve the environment faster.</p></li>
</ol>
<h3 id='a-name-hue-a-the-hue-metrics'><a name="hue"></a>The hue metrics</h3>
<p>Each data point represents a trial, with the data averaged over its sessions. The points are colored (see legend) with the hue:</p>

<ul>
<li><code>solved_ratio_of_sessions</code>: how many sessions are solved out of the total sessions in a trial, 0 means none, 1 means all.</li>
</ul>

<p>The granularity of the <code>solved_ratio_of_sessions</code> depends on the number of sessions ran per trial. From experience, we settle on 5 sessions per trial as it&#39;s the best tradeoff between granularity and computation time.</p>

<p>Multiple sessions allow us to observe the consistency of an agent. As we have noticed across the parameter space, there is a spectrum of solvability: agents who cannot solve at all, can solve occasionally, and can always solve. The agents that solves occasionally can be valuable when developing an new algorithm, and most people will throw them away - this is bad when a strong agent is hard to find in the early stage.</p>
<h3 id='how-to-read-2'>How to read</h3>
<p>Every subplot in the graph shows the distribution of all the trial points in the pair of <em>y vs x</em> variables, with the other <em>x&#39;</em> dimensions flattened. For each, observe the population distribution, y-positions, and trend across the x-axis.</p>

<p>Note that these will use <a href="http://seaborn.pydata.org/generated/seaborn.swarmplot.html">swarmplot</a> which allows us to see the distribution of points by spreading them horizontally to prevent overlap. However, when the x-axis has too many values (.e.g continuous x-values in random search), it will switch to scatter plot instead.</p>

<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582554/1fbefc98-1700-11e7-85f8-b0740b25007f.png" title="The acrobot random search analysis graph" alt="1fbefc98 1700 11e7 85f8 b0740b25007f" />
<em>An example from rand_acrobot with scatterplot instead of swarmplot for gamma and lr. This is when RandomSearch is used. The example is from the <a href="https://github.com/kengz/openai_lab/pull/94">rand_acrobot-2017_03_28_082754 PR</a></em></p>
</blockquote>

<p><strong>Population distribution</strong>: more darker points implies that the many trials could solve the environment consistently. Higher ratio of dark points also means the environment is easier for the agent. If the points are closer and the distribution has smaller vertical gaps, then the <em>x</em> is a stabler value for the <em>y</em> value even when other <em>x&#39;</em> dimensions vary. In a scatterplot, clustering of points in a random search also shows the convergence of the search.</p>

<p><strong>trend across y-values</strong>: the fitter trial will show up higher in the y-axes (except for <code>epi_stats_min</code>). Generally good solutions are scarce and they show up at higher <code>fitness_score</code>, whereas the non-solutions get clustered in the lower region. Notice how the <code>fitness_score</code> plots can clearly distinguish the good solutions (darker points), whereas in the <code>mean_rewards_stats_max</code> and <code>max_total_rewards_stats_mean</code> plots it is hard to tell apart. We will discuss how the custom-formulated <code>fitness_score</code> function achieves this in the <a href="#metrics">metrics</a> section.</p>

<p><strong>trend across x-values</strong>: to find a stable and good <em>x-value</em>, observe the vertical gaps in distribution, the clustering of darker points. Usually there&#39;s one maxima with a steady trend towards it. Recall that the plots flatten the other <em>x&#39;</em> values, but the dependence on <em>x</em> value is usually very consistent across <em>x&#39;</em> that there will still be a flattened trend.</p>
<h2 id='correlation-graph'>Correlation Graph</h2>
<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582681/be76f4ec-1702-11e7-9935-2491189ab1e6.png" title="Correlation graph" alt="Be76f4ec 1702 11e7 9935 2491189ab1e6" />
<em>The correlation graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. We can see smooth contours of spectrum in them, suggesting that the x-values are stable - small change in values will not be catastrophic. There are 2 darker regions in the contour; the distribution confirms that gamma=0.999 and lower lr are indeed stabler, since they have higher populations of darker points. The instability of gamma=0.95 shows clearly as 2 segments of stacked bar with huge contrast.</em></p>
</blockquote>

<p>The <strong>correlation graph</strong> reveals pairwise x-value correlations that is flattened in the analysis graph. This is a pair-plot between the orderable parameter variables.</p>
<h3 id='how-to-read-3'>How to read</h3>
<p>The diagonals simply shows the population distribution for that x-value; the off-diagonal plots show the fitness score heatmap that tells how to best combine separate parameter values. Note that the heatmap color scheme does not represent absolute fitness, but points are colored by which <code>fitness_score_bin</code> they fall into.</p>

<p>The points are semi-transparent, so if they overlap, their colors will stack instead of hiding the points behind.</p>
<h2 id='data'>Data</h2>
<p>After glancing through the graphs, it will be easier to understand the data and find the targets.</p>
<h3 id='how-to-read-4'>How to read</h3>
<p>The <code>&lt;experiment_id&gt;_analysis_data.csv</code> will show the data for each trial, sorted by the highest <code>fitness_score</code> first. The left columns are the measured output values; then they&#39;re separated by the <code>trial_id</code>; the right columns are the parameter values for the trial.</p>

<p>The <code>trial_id</code> will tell us which <code>trial_data</code> to check for even more details on the best trials. Usually we can also spot some trend in the right parameter columns.</p>

<p>The best <code>&lt;trial_id&gt;.json</code> will show us directly what is its <code>experiment_spec</code>, and more stats about the trial. When <a href="#solutions">submitting a solution PR</a>, retrieve the <code>experiment_spec</code> to update the default <code>*_experiment_spec.json</code>, and get the <code>fitness_score</code> from here too.</p>

<table><thead>
<tr>
<th style="text-align: left">best_session_epi</th>
<th style="text-align: left">best_session_id</th>
<th style="text-align: left">best_session_mean_rewards</th>
<th style="text-align: left">best_session_stability</th>
<th style="text-align: left">fitness_score</th>
<th style="text-align: left">mean_rewards_per_epi_stats_mean</th>
<th style="text-align: left">mean_rewards_stats_mean</th>
<th style="text-align: left">mean_rewards_stats_max</th>
<th style="text-align: left">epi_stats_mean</th>
<th style="text-align: left">epi_stats_min</th>
<th style="text-align: left">solved_ratio_of_sessions</th>
<th style="text-align: left">max_total_rewards_stats_mean</th>
<th style="text-align: left">trial_id</th>
<th style="text-align: left">variable_gamma</th>
<th style="text-align: left">variable_hidden_layers</th>
<th style="text-align: left">variable_lr</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">114</td>
<td style="text-align: left">dqn-2017_03_19_004714_t79_s2</td>
<td style="text-align: left">195.59</td>
<td style="text-align: left">0.845361</td>
<td style="text-align: left">9.635032</td>
<td style="text-align: left">1.326499</td>
<td style="text-align: left">195.404</td>
<td style="text-align: left">195.66</td>
<td style="text-align: left">154.2</td>
<td style="text-align: left">114.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t79</td>
<td style="text-align: left">0.999</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.02</td>
</tr>
<tr>
<td style="text-align: left">157</td>
<td style="text-align: left">dqn-2017_03_19_004714_t36_s4</td>
<td style="text-align: left">196.07</td>
<td style="text-align: left">1.010526</td>
<td style="text-align: left">9.282276</td>
<td style="text-align: left">1.155354</td>
<td style="text-align: left">195.602</td>
<td style="text-align: left">196.07</td>
<td style="text-align: left">169.0</td>
<td style="text-align: left">157.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t36</td>
<td style="text-align: left">0.97</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.001</td>
</tr>
<tr>
<td style="text-align: left">140</td>
<td style="text-align: left">dqn-2017_03_19_004714_t28_s0</td>
<td style="text-align: left">196.11</td>
<td style="text-align: left">0.989474</td>
<td style="text-align: left">9.178363</td>
<td style="text-align: left">1.179311</td>
<td style="text-align: left">195.564</td>
<td style="text-align: left">196.11</td>
<td style="text-align: left">167.2</td>
<td style="text-align: left">140.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t28</td>
<td style="text-align: left">0.97</td>
<td style="text-align: left">[32]</td>
<td style="text-align: left">0.001</td>
</tr>
<tr>
<td style="text-align: left">123</td>
<td style="text-align: left">dqn-2017_03_19_004714_t50_s0</td>
<td style="text-align: left">195.16</td>
<td style="text-align: left">0.96875</td>
<td style="text-align: left">9.074483</td>
<td style="text-align: left">1.276302</td>
<td style="text-align: left">195.136</td>
<td style="text-align: left">195.3</td>
<td style="text-align: left">160.6</td>
<td style="text-align: left">123.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t50</td>
<td style="text-align: left">0.99</td>
<td style="text-align: left">[32]</td>
<td style="text-align: left">0.01</td>
</tr>
<tr>
<td style="text-align: left">153</td>
<td style="text-align: left">dqn-2017_03_19_004714_t16_s2</td>
<td style="text-align: left">195.48</td>
<td style="text-align: left">1.010526</td>
<td style="text-align: left">8.98669</td>
<td style="text-align: left">1.159941</td>
<td style="text-align: left">195.466</td>
<td style="text-align: left">195.7</td>
<td style="text-align: left">168.6</td>
<td style="text-align: left">153.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t16</td>
<td style="text-align: left">0.95</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.001</td>
</tr>
</tbody></table>

<p><em><code>dqn-2017_03_19_004714_analysis_data.csv</code>, top 5 trials, from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. We can see that among the dominating parameter values are gamma=0.999, hidden_layers=[64], lr=[0.02]. The best trial json below.</em></p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"experiment_spec"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"experiment_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"decay"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="s2">"metrics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"best_session_epi"</span><span class="p">:</span><span class="w"> </span><span class="mi">114</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn-2017_03_19_004714_t79_s2"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_mean_rewards"</span><span class="p">:</span><span class="w"> </span><span class="mf">195.59</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_stability"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8453608</span><span class="p">,</span><span class="w">
    </span><span class="s2">"epi_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">154.2</span><span class="p">,</span><span class="w">
    </span><span class="s2">"fitness_score"</span><span class="p">:</span><span class="w"> </span><span class="mf">9.635032</span><span class="p">,</span><span class="w">
    </span><span class="s2">"max_total_rewards_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w">
    </span><span class="s2">"mean_rewards_per_epi_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.326499</span><span class="p">,</span><span class="w">
    </span><span class="s2">"mean_rewards_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">195.404</span><span class="p">,</span><span class="w">
    </span><span class="s2">"solved_ratio_of_sessions"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"t_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mi">199</span><span class="p">,</span><span class="w">
    </span><span class="s2">"time_taken"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0:41:19"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="err">...</span><span class="w">
</span></code></pre>
<p>This concludes the analysis. See the <a href="https://github.com/kengz/openai_lab/pull/73">solution PR here</a>. The best trial is <code>dqn-2017_03_19_004714_t79</code>, with <code>fitness_score=9.635032</code>, and the variables:</p>

<ul>
<li><em>lr</em>: 0.02</li>
<li><em>gamma</em>: 0.999</li>
<li><em>hidden_layers_shape</em>: [64]</li>
</ul>

<p>Now that you know how to analyze the data,</p>

<ul>
<li><a href="#solutions">start finding problems to beat and submitting your solutions</a></li>
<li><a href="#metrics">learn more about the evaluation metrics</a></li>
</ul>

          <h1 id='a-name-solutions-a-solutions'><a name="solutions"></a>Solutions</h1>
<p>Agents and best solutions by OpenAI Lab users. We want people to start from working solutions instead of stumbling their ways there.</p>
<h2 id='submission-instructions'>Submission instructions</h2>
<p>If you invent a new algorithm/combination that beats the best solutions, please submit a <a href="https://github.com/kengz/openai_lab/pulls">Pull Request</a> to OpenAI Lab. Refer to the <a href="https://github.com/kengz/openai_lab/blob/master/.github/PULL_REQUEST_TEMPLATE.md">PR template</a> for the submission guideline. See examples from the accepted <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>.</p>

<p>To learn how to analyze experiment data, refer to <a href="#analysis">Analysis</a>.</p>
<h2 id='a-name-fitness-matrix-a-fitness-matrix'><a name="fitness-matrix"></a>Fitness Matrix</h2>
<p>A matrix of the best <code>fitness_score</code> of <strong>Agents</strong> v.s. <strong>Environments</strong>, sourced from the accepted <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>. See <a href="#metrics">Metric</a> for the design of fitness score and generalized metrics.</p>

<table><thead>
<tr>
<th style="text-align: left"></th>
<th style="text-align: left">DQN</th>
<th style="text-align: left">DoubleDQN</th>
<th style="text-align: left">Sarsa</th>
<th style="text-align: left">OffPolicySarsa</th>
<th style="text-align: left">ActorCritic</th>
<th style="text-align: left">DDPG</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><strong>CartPole-v0</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/73">9.635032</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/78">10.34826</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/91">12.98525</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/123">13.90989</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>CartPole-v1</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/80">13.22935</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/82">16.06697</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/107">18.91624</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/124">30.57067</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Acrobot-v1</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/94">-0.1051617</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/98">-0.1045992</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/97">-0.1127294</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/128">-0.1175654</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MountainCar-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/117">-0.03744196</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MountainCarContinuous-v0</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Pendulum-v0</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/130">-0.2696825</a></td>
</tr>
<tr>
<td style="text-align: left"><strong>LunarLander-v2</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/84">2.786624</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/87">2.992104</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/96">3.313421</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>LunarLanderContinuous-v2</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/133">-0.03972519</a></td>
</tr>
<tr>
<td style="text-align: left"><strong>BipedalWalker-v2</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>BipedalWalkerHardcore-v2</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>CarRacing-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>AirRaid-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Alien-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Assault-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Breakout-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MsPacman-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Pong-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Qbert-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>SpaceInvader-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>FlappyBird-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Snake-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h2 id='a-name-agents-matrix-a-agents-fitness-matrix'><a name="agents-matrix"></a>Agents Fitness Matrix</h2>
<p>A projection of the Fitness Matrix along the Agents axis. This shows overall status of the Agents in OpenAI Lab. Feel free to invent new ones! For more details, see <a href="#algorithms">Algorithms</a> and <a href="#families">Families of RL Algorithms</a>.</p>

<p><em>Pending: we have a generic formalization to cross-evaluate Agents using heatmap statistics; see <a href="#metrics">Metrics</a>. This is on the roadmap.</em></p>

<table><thead>
<tr>
<th style="text-align: left">algorithm</th>
<th style="text-align: left">implementation</th>
<th style="text-align: left">eval score (pending)</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1312.5602">DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/dqn.py">DQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.06461">Double DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/double_dqn.py">DoubleDQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.06581">Dueling DQN</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/deep_sarsa.py">DeepSarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Off-Policy Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/offpol_sarsa.py">OffPolicySarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.05952">PER (Prioritized Experience Replay)</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/memory/prioritized_exp_replay.py">PrioritizedExperienceReplay</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">CEM (Cross Entropy Method)</a></td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://incompleteideas.net/sutton/williams-92.pdf">REINFORCE</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf">DPG (Deterministic Policy Gradient) off-policy actor-critic</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/actor_critic.py">ActorCritic</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.02971">DDPG (Deep-DPG) actor-critic with target networks</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/ddpg.py">DDPG</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/pdf/1602.01783.pdf">A3C (asynchronous advantage actor-critic)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Dyna</td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1502.05477">TRPO</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Q*(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Retrace(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control (NEC)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1612.00796">EWC (Elastic Weight Consolidation)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h2 id='a-name-environments-matrix-a-environments-fitness-matrix'><a name="environments-matrix"></a>Environments Fitness Matrix</h2>
<p>A projection of the Fitness Matrix along the Environments axis. This shows the best solutions for the environments. The list of accepted solutions can be seen in the <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>.</p>
<h3 id='classic-environments'>Classic Environments</h3>
<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">CartPole-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/123">13.90989</a></td>
<td style="text-align: left">3</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">cartpole_ac_softmax</td>
</tr>
<tr>
<td style="text-align: left">CartPole-v1</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/124">30.57067</a></td>
<td style="text-align: left">3</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">cartpole_v1_ac_softmax</td>
</tr>
<tr>
<td style="text-align: left">Acrobot-v1</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/98">-0.1045992</a></td>
<td style="text-align: left">-104.34</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">acrobot_offpol_sarsa</td>
</tr>
<tr>
<td style="text-align: left">MountainCar-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/117">-0.03744196</a></td>
<td style="text-align: left">970</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">mountain_double_dqn</td>
</tr>
<tr>
<td style="text-align: left">MountainCarContinuous-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Pendulum-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/130">-0.2696825</a></td>
<td style="text-align: left">-137.3549</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">pendulum_ddpg_per_linearnoise</td>
</tr>
</tbody></table>
<h3 id='box2d-environments'>Box2D Environments</h3>
<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">LunarLander-v2</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/96">3.313421</a></td>
<td style="text-align: left">200</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">lunar_offpol_sarsa</td>
</tr>
<tr>
<td style="text-align: left">LunarLanderContinuous-v2</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/133">-0.03972519</a></td>
<td style="text-align: left">503</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">lunar_cont_ddpg_linearnoise</td>
</tr>
<tr>
<td style="text-align: left">BipedalWalker-v2</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">BipedalWalkerHardcore-v2</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">CarRacing-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h3 id='atari-environments'>Atari Environments</h3>
<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">AirRaid-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Alien-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Assault-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Breakout-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">MsPacman-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Pong-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Qbert-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">SpaceInvader-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h3 id='pygame-environments'>PyGame Environments</h3>
<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">FlappyBird-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Snake-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>
<h3 id='universe-environments'>Universe Environments</h3>
<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

          <h1 id='a-name-metrics-a-metrics'><a name="metrics"></a>Metrics</h1>
<p>The Lab setup allows us to run experiments at scale; the standardized framework also allows us to reliably compare multiple agents (algorithms) and environments (problems). These are shown with the <a href="#fitness-matrix">Fitness Matrix</a>, which also necessitates a higher level evaluation metric.</p>

<p>With the Lab, we are breeding multiple agents across many environments and selecting the best ones. Naturally, this selection metric is called the <code>fitness_score</code>. Some evolutionary search algorithm for the <code>HyperOptimizer</code> is on our <a href="#roadmap">roadmap</a>.</p>

<p>The Fitness Matrix is a projection from the parameter space of each agent-environment pair, where each matrix cell is the highest fitness score the agent could achieve in the environment.</p>

<p>To understand the bigger picture, the domain for the fitness function for each matrix cell is the parameter space of the agent conditioned on the environment. Inside the parameter space, each point gets mapped to a fitness score.</p>

<p>To analogize, see fitness score as temperature, then we have a heatmap inside the parameter space, and we are searching for the hottest point and recording that in a cell of the Fitness Matrix.</p>

<p>In this section, we will formalize these ideas.</p>
<h2 id='a-name-fitness-a-fitness-score'><a name="fitness"></a>Fitness Score</h2>
<p>The fitness function <code>f</code> is the base function behind the Fitness Matrix and the fitness heatmap. It computes the fitness score for each point (trial) in a parameter space.</p>

<p>The fitness function&#39;s design is motivated by the need to have a richer evaluation of an agent, and the ability of the Lab to provide such data. We felt that the <code>mean rewards</code> metric used in the OpenAI gym evaluation is insufficient, but it is included in our design under <em>strength</em>.</p>

<p>The fitness score of a trial is as follows:</p>

<p><code>fitness_score = mean_rewards_per_epi_mean * [(1+stability_mean) * ((1+consistency)**2)] ** sign(mean_rewards_per_epi_mean)</code></p>

<p>or renaming variables by what the terms represent:</p>

<p><code>fitness_score = power * distinguisher</code></p>

<p>where</p>

<ul>
<li><code>power = mean_rewards_per_epi_mean</code></li>
<li><code>distinguisher = amplifier ** sign(power)</code></li>
<li><code>amplifier = (1+stability_mean) * ((1+consistency)**2)</code></li>
</ul>

<p>The fitness score is designed to capture the following:</p>

<ol>
<li><strong>strength</strong>: <code>mean_rewards</code> (over the past 100 episodes)</li>
<li><strong>speed</strong>: <code>1/epi</code></li>
<li><strong>stability</strong>: session-level stability, <code>stable_epi_count / mastery_gap</code></li>
<li><strong>consistency</strong>: AKA trial-level stability, <code>solved_ratio_of_session</code></li>
<li><strong>granularity</strong>: in <code>1+stability</code> and <code>1+consistency</code> to ensure partial solution doesn&#39;t get lost when stability and consistency = 0</li>
<li><strong>amplification</strong>: amplify by session-level stability linearly with <code>*(1+stability)</code>, and by trial-level stability quadratically with <code>*(1+consistency)**2</code></li>
<li><strong>distinguishability</strong>: multiply amplifier if <code>power</code> is positive, else divide; essentially <code>*(amplifier**sign(power))</code></li>
</ol>
<h3 id='strength'>Strength</h3>
<p>The higher (more positive) the <code>mean_rewards</code> an agent gets, the stronger it is. The mean is taken over the past 100 episodes, as common to OpenAI gym.</p>
<h3 id='speed'>Speed</h3>
<p>Given two same <code>mean_rewards</code>, the agent that achieves it in less episodes is faster, with <code>speed = 1/epi</code>. This yields the notion of <code>power = strength * speed = mean_rewards_per_epi</code>. Use the sessions-mean of a trial, i.e. <code>mean_rewards_per_epi_mean</code> = mean of multiple <code>mean_rewards_per_epi</code> values.</p>
<h3 id='stability'>Stability</h3>
<p><code>stability = stable_epi_count / mastery_gap</code> for a session, with range <code>0.0 - 1.0</code> from unstable to stable. Measures session-level stability. Use the sessions-mean of a trial, i.e. mean of multiple <code>stability</code> values.</p>

<ul>
<li><code>stable_epi_count</code> = the number of stable episodes, where stable means an episode passes the <code>r_threshold</code>.</li>
<li><code>r_threshold</code> = <code>sys_vars[&#39;SOLVED_MEAN_REWARD&#39;]</code> if the problem is solvable; <code>max_rewards - (0.10 * (max_rewards-min_rewards))</code> otherwise.</li>
<li><code>first_solved_epi</code> = first episode that passes the <code>r_threshold</code>.</li>
<li><code>mastery_gap</code> = <code>last_epi - first_solved_epi</code> normally, or <code>INF</code> to yield stability 0.0 if <code>first_solved_epi</code> is undefined because no episode passes the threshold.</li>
</ul>

<p>The reasoning for the definition is simple: regardless if the problem is solved or not, we want to see if the agent could stability its performance. Once it solves (for solvable problems) or achieves a sufficiently high score (unsolved problems), its <code>mean_rewards</code> every episode should not drop too much, even when accounting for random fluctuations. We can define a minimal threshold (10% less than the ideal) that the subsequent episodes should surpass. Then, the stability is simple the ratio of the number of episodes that pass the threshold after the first solution.</p>
<h3 id='consistency'>Consistency</h3>
<p><code>consistency = solved_ratio_of_session</code> for a trial, with range <code>0.0 - 1.0</code> from inconsistent to consistent. This is the trial-level measurement of stability, as it measures how consistently the agent can solve an environment given multiple repeated sessions.</p>

<p><code>consistency = 0</code> always for unsolved problems (unbounded rewards) since solution is undefined.</p>
<h3 id='granularity'>Granularity</h3>
<p>When <code>stability=0</code> or <code>consistency=0</code> the multiplier will drop to zero, regardless if there is any partial solutions. This will make training harder as the agent will not learn from partial solutions if these are treated as non-solutions. So, restore the granularity that preserves partial solution simply by adding 1, i.e. <code>1+stability</code> and <code>1+consistency</code>.</p>
<h3 id='amplification'>Amplification</h3>
<p>To separate solutions from noise, amplify the good ones and separate them out, while diminish and cluster the worse ones together. Amplify by session-level stability linearly with <code>*(1+stability)</code> since it&#39;s of the same order as <code>power</code>. Amplify by trial-level stability quadratically with <code>*(1+consistency)**2</code> since trial stability is of the next order. Amplifier is always positive.</p>
<h3 id='distinguishability'>Distinguishability</h3>
<p>Always amplify to make better solutions have more positive fitness score. If <code>power</code> is negative, amplify toward the 0 axis, i.e. divide by amplifier. If <code>power</code> is positive, amplify away from the 0 axis, i.e. multiply the amplifier. Essentially, <code>distinguisher = amplifier**sign(power)</code>.</p>
<h2 id='generalized-metrics'>Generalized Metrics</h2>
<p>With the fitness function defined above, we can evaluate a single agent in a single environment. In fact, this is a single experiment with multiple trials, and we compute a fitness score per trial, using the trial&#39;s parameter values. We then pick the max fitness score for the Fitness Matrix.</p>

<p>Given that the Lab can run multiple agents across environments in its standardized framework, naturally we ask:</p>

<ul>
<li>we can evaluate <strong>one agent in one environment</strong>, (experiment metrics)</li>
<li>how do we evaluate and compare <strong>multiple agents in one environment</strong>? (environment metrics)</li>
<li>how about <strong>one agent in multiple environments</strong>? (agent metrics)</li>
<li>how about the universal-view of <strong>multiple agents in multiple environments</strong>? (universal metrics)</li>
</ul>

<p>This section formalizes the generalized metrics for these, and shows that the Fitness Matrix is just a max-function projection of some higher dimensional space.</p>
<h3 id='a-name-generalization-a-generalization'><a name="generalization"></a>Generalization</h3>
<p>We include the formalization of evaluation metrics across the Agent space and Environment space, and present a generalization, which produces the <a href="#fitness-matrix">Fitness Matrix</a>.</p>

<p><em>We&#39;re using LaTeX for better symbolic rendering.</em></p>

<p><img src="images/metric_1.png" alt="Metric 1" />
<img src="images/metric_2.png" alt="Metric 2" />
<img src="images/metric_3.png" alt="Metric 3" />
<img src="images/metric_4.png" alt="Metric 4" /></p>

          <h1 id='a-name-algorithms-a-algorithms-theory'><a name="algorithms"></a>Algorithms (Theory)</h1>
<p>The currently implemented algorithms combine deep neural networks with a number of classic reinforcement learning algorithms. These are just a starting point. Please invent your own! </p>
<h2 id='what-is-reinforcement-learning'>What is reinforcement learning?</h2>
<p><em>Reinforcement learning (RL) is learning from interaction with an environment, from the consequences of action, rather than from explicit teaching. RL become popular in the 1990s within machine learning and artificial intelligence, but also within operations research and with offshoots in psychology and neuroscience.</em></p>

<p><em>Most RL research is conducted within the mathematical framework of Markov decision processes (MDPs). MDPs involve a decision-making agent interacting with its environment so as to maximize the cumulative reward it receives over time. The agent perceives aspects of the environment&#39;s state and selects actions. The agent may estimate a value function and use it to construct better and better decision-making policies over time.</em></p>

<p><em>RL algorithms are methods for solving this kind of problem, that is, problems involving sequences of decisions in which each decision affects what opportunities are available later, in which the effects need not be deterministic, and in which there are long-term goals. RL methods are intended to address the kind of learning and decision making problems that people and animals face in their normal, everyday lives.</em></p>

<p><em>- Rich Sutton</em></p>

<p>For further reading on reinforcement learning see <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver&#39;s lectures</a> and the book, <a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction, by Sutton and Barto</a>.</p>
<h2 id='why-are-deep-neural-networks-useful-for-solving-rl-problems'>Why are deep neural networks useful for solving RL problems?</h2>
<p>RL problems are characterized by incomplete information. The transition probabilities from one state to another given the action taken, for all states and actions are not known. Nor is the distribution of the rewards given a state and action. So in order to solve problems, RL algorithms involve approximating one or more unknown, typically complex, non linear functions. Deep neural networks make good candidates for these function approximators since they excel at approximating complex functions, particularly if the states are characterized by pixel level features.</p>

<p>For further reading on neural networks see <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></p>
<h2 id='terminology'>Terminology</h2>
<ul>
<li>Agent: encapsulates a specific algorithm. Each agent has a policy, memory, optimizer, and preprocessor</li>
<li>Online training: agents are trained periodically during episodes</li>
<li>Episodic training: agents are only trained after an episode has completed and before the next episode begins</li>
<li>Policy: rule which determines how to act in a given state, e.g. choose the action A which has the highest Q-value in state <code>S</code>. May be deterministic or stochatic. </li>
<li>Q function: <code>Q(S, A)</code>, estimates the value of taking action <code>A</code> in state <code>S</code> under a specific policy. </li>
<li>Q-vallue: Value of <code>Q</code> function for a particular <code>S</code> and <code>A</code>.</li>
<li>On policy: the same policy is used to act and evaluate the quality of actions.</li>
<li>Off policy: a different policy is used to act and evaluate the quality of actions. </li>
</ul>
<h2 id='a-name-families-a-families-of-rl-algorithms'><a name="families"></a>Families of RL Algorithms</h2>
<p>To navigate the different RL algorithms:</p>

<ul>
<li><p>Model-free</p>

<ul>
<li>Value-based

<ul>
<li><strong>DQN</strong></li>
<li><strong>DoubleDQN</strong></li>
<li><strong>Sarsa</strong></li>
<li><strong>OffpolSarsa</strong></li>
</ul></li>
<li>Policy-based

<ul>
<li><strong>REINFORCE</strong></li>
</ul></li>
<li>ActorCritic

<ul>
<li><strong>DPG</strong> - typically implemented with ActorCritic, there&#39;s a different policy for exploration and target</li>
<li><strong>DDPG</strong> - very similar to DPG except with target networks and batch normalization</li>
</ul></li>
</ul></li>
<li><p>Model based</p>

<ul>
<li>Value based

<ul>
<li><strong>Dyna</strong></li>
</ul></li>
<li>Policy based

<ul>
<li><em>pending</em></li>
</ul></li>
<li>ActorCritic

<ul>
<li><em>pending</em></li>
</ul></li>
</ul></li>
</ul>
<h2 id='implemented-algorithms'>Implemented Algorithms</h2>
<p><em>See the overall list of agents and their implementation status under the <a href="#agents-matrix">Agents Fitness Matrix</a>. This section briefly explains the theories behind the implemented algorithms/agents.</em></p>
<h3 id='a-name-q-learning-a-q-learning'><a name="q-learning"></a>Q-Learning</h3>
<p>Q-learning algorithms attempt to estimate the optimal Q function, i.e the value of taking action <code>A</code> in state <code>S</code> under a specific policy. Q-learning algorithms have an implicit policy, typically <code>epsilon</code>-greedy in which the action with the maximum <code>Q</code> value is selected with probability <code>(1 - epsilon)</code> and a random action is taken with probability <code>epsilon</code>. The random actions encourage exploration of the state space and help prevent algorithms from getting stuck in local minima.</p>

<p>Q-learning algorithms are off-policy algorithms in that the policy used to evaluate the value of the action taken is different to the policy used to determine which state-action pairs are visited.</p>

<p>It is also a temporal difference algorithm. Updates to the <code>Q</code> function are based on existing estimates. The estimate in time t is updated using an estimate from time t+1. This allows Q-Learning algorithms to be online and incremental, so the agent can be trained during an episode. The update to <code>Q_t(S, A)</code> is as follows</p>

<p><img src="images/q_learning.png" alt="Q learning" /></p>

<p>For more details, please see chapter 6 of <a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction, Sutton and Barto</a>.</p>

<p>Since the policy that is used to evaluate the target is fixed (a greedy policy that selects the action that maximises the Q-value for a particular state) and is different to the policy used to determine which state-action pairs are being visited, it is possible to use experience replay to train an agent.</p>

<p>This is often needed for Agents to act in environments to get experiences. Experiences consist of the state, the action taken, the next state, and the reward, and are denoted as <code>&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;</code>. These experiences are stored in the agents memory. Periodically during an episode the agent is trained. During training n batches of size m are selected from memory and the <code>Q</code> update step is performed. This is different to Sarsa algorithms which are on-policy and agents are trained after each experience using only the most recent experience.</p>

<p><strong>Deep Q-Learning</strong></p>

<p>Standard Q-learning algorithm with experience replay. Online training every <code>n</code> experiences.</p>

<p><img src="images/deep_q_learning.png" alt="Deep q learning" /></p>

<p>Agents: </p>

<ul>
<li><code>DQN</code>: function approximator - feedforward neural network</li>
<li><code>ConvDQN</code>: function approximator - convolutional network</li>
</ul>

<p><strong>Double Q-Learning</strong></p>

<p>Q-learning algorithm with two <code>Q</code> function approximators to address the maximisation bias problem, <code>Q_1</code>, and <code>Q_2</code>. One <code>Q</code> function is used to select the action in the next state, <code>S&#39;</code>, the other is used to evaluate the action in state <code>S&#39;</code>. Periodically the roles of each <code>Q</code> function are switched. Online training every <code>n</code> experiences.</p>

<p><img src="images/double_q_learning.png" alt="Double q learning" /></p>

<p>Agents:</p>

<ul>
<li><code>DoubleDQN</code>: function approximator - feedforward neural network</li>
<li><code>DoubleConvQN</code>: function approximator - convolutional network</li>
</ul>

<p><strong>Deep Q-Learning with weight freezing</strong></p>

<p>Deep Q-Learning algorithms tends to be unstable. To address this issue, create two <code>Q</code> function approximators, one for exploration, <code>Q_e</code>, and one for evaluating the target, <code>Q_t</code>. The target is a copy of the exploration network with frozen weights which lag the exploration network.</p>

<p>These weights are updated periodically to match the exploration network. Freezing the target network weights help avoids oscillations in the policy, where slight changes to Q-values can lead to significant changes in the policy, and helps break correlations between the Q-network and the target. See <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf">David Silver&#39;s</a> lecture slides for more details. Online training every n experiences.</p>

<p><img src="images/freeze_dqn.png" alt="Freeze dqn" /></p>

<p>Agents:</p>

<ul>
<li><code>FreezeDQN</code>: function approximator - feedforward neural network</li>
</ul>
<h3 id='a-name-sarsa-a-sarsa'><a name="sarsa"></a>Sarsa</h3>
<p><code>Sarsa</code> algorithms also attempt to estimate the optimal <code>Q</code> function. They are on policy algorithms so the policy used to evaluate the target is the same as to the policy used to determine which state-action pairs are being visited.</p>

<p>Like Q-Learning, <code>Sarsa</code> is a temporal difference algorithm. However, since they are on policy, it is trickier to take advantage of experience replay, requiring storage of the action in state <code>t+1</code> and the Q-value for the state and action selected in <code>t+1</code> in an experience. In the following implementations, updates are made after each action with the exception of off policy expected Sarsa.</p>

<p>Sarsa update:</p>

<p><img src="images/sarsa.png" alt="Sarsa" /></p>

<p>This update is made each time the agent acts in an environment and gets an experience <code>&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;</code></p>

<p><strong>Sarsa</strong></p>

<p>Standard Sarsa algorithm</p>

<p><img src="images/deep_sarsa.png" alt="Deep sarsa" /></p>

<p>Agents:</p>

<ul>
<li><code>DeepSarsa</code>: function approximator - feedforward neural network</li>
</ul>

<p><strong>Expected Sarsa</strong></p>

<p>Uses the expected value of the <code>Q</code> function under the current policy to construct the target instead of the Q-value for the action selected.</p>

<p><img src="images/expected_sarsa.png" alt="Expected sarsa" /></p>

<p>Agents</p>

<ul>
<li><code>DeepExpectedSarsa</code>: function approximator - feedforward neural network</li>
</ul>

<p><strong>Off Policy Expected Sarsa</strong></p>

<p>Sarsa is typically an on policy algorithm. However, if a different policy is used to evaluate the target than the one used to explore, it becomes and off-policy algorithm. With this set up, Q-Learning can be understood as a specific instance of Off Policy Expected Sarsa, when the policy used to evaluate the target is the greedy policy.</p>

<p>In off policy expected sarsa, actions are selected under the exploration policy, (annealling epsilon greedy for example).  Then the value of next state and action pair is calculated as the expected Q value under the target policy, for example epsilon greedy with some fixed value for epsilon.</p>

<p><code>Q</code> update and translation to neural network update: Same as <code>ExpectedSarsa</code> with fixed epsilon.</p>

<p>Agents:</p>

<ul>
<li><code>OffPolicySarsa</code>: function approximator - feedforward neural network</li>
</ul>
<h3 id='a-name-pg-a-policy-gradient'><a name="pg"></a>Policy Gradient</h3>
<p><em>In progress</em></p>
<h3 id='a-name-ddpg-a-deep-deterministic-policy-gradient'><a name="ddpg"></a>Deep Deterministic Policy Gradient</h3>
<p><em>In progress</em></p>
<h2 id='rl-theory-resources'>RL Theory Resources</h2>
<ul>
<li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a>: an introductory blog post about deep reinforcement learning; a good and short first intro for new comers.</li>
<li><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo">Deep Reinforcement Learning</a>: 90 minute video overview by John Schulman. Assumes some knowledge of neural networks. If you are not familiar with neural networks, then start with Sutton and Barto’s book or David Silver&#39;s course</li>
<li><a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction</a>: Classic textbook by Sutton and Barto. A good place to go next after watching John Schulman’s talk. Link is to the in progress 2nd edition. </li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver’s Reinforcement Learning Course</a></li>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning, 2013</a>: The famous paper from DeepMind describing a convolutional neural network Q-Learning architecture which learned to play seven different Atari games, achieving superhuman performance in three of them.</li>
</ul>

          <h1 id='a-name-agents-a-agents-code'><a name="agents"></a>Agents (Code)</h1>
<p>Agents are containers for reinforcement learning algorithms. They consist of a number of different components which are specified in the <code>experiment_specs</code>.</p>

<blockquote>
<p>The corresponding source code folder. When composing an agent by choosing components, look at the source code folder to see the list of available <code>Classes</code> to use, and look at their <code>__init__</code> method to see what parameters are available.</p>
</blockquote>
<pre class="highlight json tab-json"><code><span class="s2">"experiment_spec"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/spec/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/spec/problems.json"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/agent/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/policy/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/memory/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/optimizer/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/hyperoptimizer/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/preprocessor/"</span><span class="p">,</span><span class="w">
</span></code></pre>
<ul>
<li><code>problem</code>: the gym environment the agent is solving.</li>
<li><code>Agent</code> (Learning algorithm),: decision function for learning from experiences gained by acting in an environment (eg Q-Learning, Sarsa). This is also the main class for agents. All other components of an agent are contained within this class.</li>
<li><code>Policy</code>: decision function for acting in an environment. Controls exploration vs. exploitation trade off(e.g. epsilon greedy, boltzmann)</li>
<li><code>Memory</code>: for storing experiences gained by acting in an environment. Controls how experiences are sampled for an agent to learn from. (e.g. random uniform with no forgetting, prioritized sampling with forgetting)</li>
<li><code>Optimizer</code>: controls how to optimize the function approximators contained within the agent (e.g. Stochatic Gradient Descent, Adam) </li>
<li><code>HyperOptimizer</code>: hyperparameter optimization algorithms used to vary the agent parameters and run trials with them (e.g grid search, random search)</li>
<li><code>PreProcessor</code>: controls the transformations made to state representaions before being passed as inputs to the policy and learning algorithm. (e.g. no preprocessing, concatenating current and previous state). Useful for Atari.</li>
<li><code>param</code>: the default parameters, unified for all agent components. To know what parameters are available, look into the <code>__init__</code> method of the components you&#39;re using. We have ensured there are no conflicting keys.</li>
<li><code>param_range</code>: the parameter space specification for the HyperOptimizer to search over in an experiment with many trials.</li>
</ul>

<p>To define an <code>Agent</code> you must specify each of the components. The example below is from the specification for <code>dqn</code> in <code>rl/spec/classic_experiment_specs.json</code>. The <code>rl/spec/*.json</code> files contains lots of other examples.</p>
<pre class="highlight json tab-json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>Each of the components with the exception of <code>Agent</code> and <code>Policy</code> are uncoupled, so can be freely switched in an out for different types of components. Different combinations of components may work better than others. We leave that up to you to experiment with. For some inspiration, see how the Lab users build <a href="#solutions">the best solutions</a>.</p>

<p>For the currently implemented algorithms, the following <code>Agents</code> can go with the following <code>Policies</code>.</p>

<ul>
<li><code>DQN, Sarsa, ExpectedSarsa, OffPolSarsa, FreezeDQN: EpsilonGreedyPolicy, OscillatingEpsilonGreedyPolicy, TargetedEpsilonGreedyPolicy, BoltzmannPolicy</code></li>
<li><code>DoubleDQN: DoubleDQNPolicy, DoubleDQNBoltzmannPolicy</code></li>
</ul>
<h2 id='problem'>problem</h2>
<p><strong>code location:</strong> <code>rl/spec/problems.json</code></p>

<p>Problems are not part of agent, but they are part of the <code>experiment_spec</code> that gets specified with the agent.</p>

<p>We have not added all the OpenAI gym environments AKA problems. If you get to new environments using the lab, please add them in <code>rl/spec/problems.json</code>, and it should be clear from those examples.</p>

<p>Moreover, when adding new problems, consider the dependencies setup too, such as Mujoco. Please add these to the <code>bin/setup</code> so that other users could run it.</p>
<h2 id='agent'>Agent</h2>
<p><strong>code location:</strong> <code>rl/agent/</code></p>

<p><em>See the overall list of agents and their implementation status under the <a href="#agents-matrix">Agents Fitness Matrix</a>. See <a href="#algorithms">algorithms section</a> for an explanation of the currently implemented agents (learning algorithms).</em></p>

<p>The main <code>Agent</code> class that will house all the other components. This is where the algorithms like <code>DQN, DDQN, DDPG</code> are implemented. This class shall contain only the algorithm-specific logic; the generic components are modularized for reuse across different agents.</p>

<p>Despite the multitude of parameters you can see in the contructor of the Agent classes, <code>Agent</code>s always need the following:</p>

<ul>
<li><code>gamma</code>: how much to discount the future</li>
<li><code>hidden_layers</code>: list of hidden layer sizes for neural network function approximators</li>
<li><code>hidden_layers_activation</code>: activation function for the hidden layers</li>
</ul>

<p>The input and output layer sizes are autoamtically inferred from the environment specs.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">],</span><span class="w">
    </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>For HyperOptimizer to search over the network architecture (yes this is possible), use the <em>auto-architecture</em> mode for building the network. The HyperOptimizer will search over the network depth and width (2 parameters is sufficient to construct a whole network). To do so, set the <code>auto-architecture</code> param to true, and specific the <code>num_hidden_layers</code> and the <code>first_hidden_layer_size</code>.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"auto_architecture"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
    </span><span class="s2">"num_hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
    </span><span class="s2">"first_hidden_layer_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre><h2 id='policy'>Policy</h2>
<p><strong>code location:</strong> <code>rl/policy/</code></p>

<p>A policy is a decision function for acting in an environment. Policies take as input a description of the state space and output an action for the agent to take.</p>

<p>Depending on the algorithm used, agents may directly approximate the policy (policy based algorithms) or have an indirect policy, that depends on the Q-value function approximation (value based algorithms). Algorithms that approximate both the policy and the Q-value function are known as actor-critic algorithms.</p>

<p>All of the algorithms implemented so far are value-based. The policy for acting at each timestep is often a simple epsilon-greedy policy.</p>

<p><img src="images/e_greedy.png" alt="E greedy" /></p>

<p>Alternatively, an indirect policy may use the Q-value to output a probability distribution over actions, and sample actions based on this distribution. This is the approach taken by the Boltzmann policies.</p>

<p>A critical component of the policy is how is balances <em>exploration</em> vs. <em>exploitation</em>. To learn how to act well in an environment an agent must <em>explore</em> the state space. The more random the actions an agent takes, the more it explores. However, to do well in an environment, an agent needs to take the best possible action given the state. It must <em>exploit</em> what it has learnt.</p>

<p>Below is a summary of the currently implemented policies. Each takes a slightly different approach to balancing the exploration-exploitation problem.</p>
<h3 id='epsilongreedypolicy'>EpsilonGreedyPolicy</h3>
<p>Parameterized by starting value for epsilon (<code>init_e</code>), min value for epsilon (<code>final_e</code>), and the number of epsiodes to anneal epsilon over (<code>exploration_anneal_episodes</code>). The value of epsilon is decayed linearly from start to min.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"EpsilonGreedyPolicy"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"init_e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="s2">"final_e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre><h3 id='doubledqnpolicy'>DoubleDQNPolicy</h3>
<p>When actions are not random this policy selects actions by summing the outputs from each of the two Q-state approximators before taking the max of the result. Same approach as <code>EpsilonGreedyPolicy</code> to decaying epsilon and same params.</p>
<h3 id='boltzmannpolicy'>BoltzmannPolicy</h3>
<p>Parameterized by the starting value for tau (<code>init_tau</code>), min value for tau (<code>final_tau</code>), and the number of epsiodes to anneal epsilon over (<code>exploration_anneal_episodes</code>). At each step this policy selects actions based on the following probability distribution</p>

<p><img src="images/boltzmann.png" alt="Boltzmann" /></p>

<p>Tau is decayed linearly over time in the same way as in the <code>EpsilonGreedyPolicy</code>.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"init_tau"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="s2">"final_tau"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre><h3 id='doubledqnboltzmannpolicy'>DoubleDQNBoltzmannPolicy</h3>
<p>Same as the Boltzmann policy except that the Q value used for a given action is the sum of the outputs from each of the two Q-state approximators.</p>
<h3 id='targetedepsilongreedypolicy'>TargetedEpsilonGreedyPolicy</h3>
<p>Same params as epsilon greedy policy. This policy swtches between active and inactive exploration cycles controlled by partial mean rewards and it distance to the target mean rewards.</p>
<h3 id='decayingepsilongreedypolicy'>DecayingEpsilonGreedyPolicy</h3>
<p>Same params as epsilon greedy policy. Epsilon is decayed exponentially.</p>
<h3 id='oscillatingepsilongreedypolicy'>OscillatingEpsilonGreedyPolicy</h3>
<p>Same as epsilon greedy policy except at episode 18 epsilon is dropped to the max of 1/3 or its current value or min epsilon.</p>
<h3 id='creating-your-own'>Creating your own</h3>
<p>A policy has to have the following functions. You can create your own by inheriting from Policy or one of its children.</p>
<pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">'''Returns the action selected given the state'''</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sys_vars</span><span class="p">):</span>
    <span class="s">'''Update value of policy params (e.g. epsilon)
    Called each timestep within an episode'''</span>
</code></pre><h2 id='memory'>Memory</h2>
<p><strong>code location:</strong> <code>rl/memory/</code></p>

<p>The agent&#39;s memory stores experiences that an agent gains by acting within an environment. An environment is in a particular state. Then the agent acts, and receives a reward from the environment. The agent also receives information about the next state, including a flag indicating whether the next state is the terminal state. Finally, an error measure is stored, indicating how well an agent can estimate the value of this particular transition. </p>

<p>This information about a single step is stored as an experience. Each experience consists of</p>

<ul>
<li>Current state</li>
<li>Action taken</li>
<li>Reward</li>
<li>Next state</li>
<li>Terminal</li>
<li>Error</li>
</ul>
<pre class="highlight python tab-python"><code><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre>
<p>Crucially, the memory controls how long experiences are stored for, and which experiences are sampled from it to use as input into the learning algorithm of an agent. Below is a summary of the currently implemented memories.</p>
<h3 id='linearmemory'>LinearMemory</h3>
<p>The size of the memory is unbounded and experiences are sampled random uniformly from memory.</p>
<h3 id='linearmemorywithforgetting'>LinearMemoryWithForgetting</h3><pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"max_mem_len"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>Parameterizes by <code>max_mem_len</code> param which bounds the size of the memory. Once memory reaches the max size, the oldest experiences are deleted from the memory to make space for new experiences. Experiences are sampled random uniformly from memory.</p>
<h3 id='lefttailmemory'>LeftTailMemory</h3>
<p>Like linear memory with sampling via a left-tail distribution. This has the effect of drawing more from newer experiences.</p>
<h3 id='prioritizedexperiencereplay'>PrioritizedExperienceReplay</h3>
<p>Experiences are weighted by the error, a measure of how well the learning algorithm currently performs on that experience. Experiences are sampled from memory in proportion to the p value (adjusted error value)</p>
<pre class="highlight python tab-python"><code><span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e</span><span class="p">)</span><span class="o">**</span> <span class="n">alpha</span>
</code></pre>
<p>The parameter <code>e &gt; 0</code> (if not positive, our implementation will bump it up) is a constant added onto the error to prevent experiences with error 0 never being sampled. <code>alpha</code> controls how spiked the distribution is. The lower <code>alpha</code> the closer to unform the distribution is. <code>alpha = 0</code> corresponds to uniform random sampling.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"PrioritizedExperienceReplay"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w">
    </span><span class="s2">"alpha"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.6</span><span class="p">,</span><span class="w">
    </span><span class="s2">"max_mem_len"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>This has the effect of drawing more from experiences that the learning algorithm doesn&#39;t perform well on, i.e. the experiences from which is has most to learn. The size of the memory is bounded as in LinearMemoryWithForgetting.</p>
<h3 id='rankedmemory'>RankedMemory</h3>
<p>Memory sorted by the <code>mean_rewards</code> of each episode. Still experimental.</p>
<h3 id='creating-your-own-2'>Creating your own</h3>
<p>A memory has to have the following functions. You can create your own by inheriting from Memory or one of its children.</p>
<pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">add_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
    <span class="s">'''add an experience to memory'''</span>

<span class="k">def</span> <span class="nf">get_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inds</span><span class="p">):</span>
    <span class="s">'''get a batch of experiences by indices
       helper function called by rand_minibatch'''</span>

<span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">'''get the last experience (batched like get_exp()'''</span>

<span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">'''returns the size of the memory'''</span>

<span class="k">def</span> <span class="nf">rand_minibatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="s">'''returns a batch of experiences sampled from memory'''</span>
</code></pre><h2 id='optimizer'>Optimizer</h2>
<p><strong>code location:</strong> <code>rl/optimizer/</code></p>

<p>Controls how to optimize the function approximators contained within the agent. For feedforward and convolutional neural networks, we suggest using Adam with the default parameters for everything except the learning rate as this is widely considered to be the best algorithm for optmizing deep neural network based function approximators. For recurrent neural networks we suggest using RMSprop.</p>
<h3 id='sgd'>SGD</h3>
<p>Stochastic Gradient Descent. Parameterized by <code>lr</code> (learning rate), <code>momentum</code>, <code>decay</code> and <code>nestorov</code>. See <a href="https://keras.io/optimizers/#sgd">Keras</a> for more details.</p>
<pre class="highlight json tab-json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"SGDOptimizer"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
    </span><span class="s2">"momentum"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.9</span><span class="w">
    </span><span class="s2">"decay"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.00001</span><span class="w">
    </span><span class="s2">"nesterov"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre><h3 id='adam'>Adam</h3>
<p>Parameterized by <code>lr</code> (learning rate), <code>beta_1</code>, <code>beta_2</code>, <code>epsilon</code>, <code>decay</code>. See <a href="https://keras.io/optimizers/#adam">Keras</a> for more details.</p>
<h3 id='rmsprop'>RMSprop</h3>
<p>Parameterized by <code>lr</code> (learning rate), <code>rho</code>, <code>epsilon</code>, <code>decay</code>. See <a href="https://keras.io/optimizers/#rmsprop">Keras</a> for more details.</p>
<h2 id='a-name-hyperoptimizer-a-hyperoptimizer'><a name="hyperoptimizer"></a>HyperOptimizer</h2>
<p><strong>code location:</strong> <code>rl/hyperoptimizer/</code></p>

<p>Controls how to search over the hyperparameter space. We suggest using each of the three hyperoptimizers in the following order when trying to find the optimal parameters for an agent in an environment, to help gradually narrow down the best search space. From rough to fine granularity, <code>LineSearch &gt; GridSearch &gt; RandomSearch</code>.</p>
<h3 id='linesearch'>LineSearch</h3>
<p>Varies one parameter per trial while using default values for the rest. This is useful when trying to estimate of what value range should work for a parameter without having to search through the full space blindly.</p>

<p>Given that we have built a proven base of best parameters, this is now seldom used, as we often could know the optimal parameter values to a small range.</p>
<h3 id='gridsearch'>GridSearch</h3>
<p>Once the search space is narrowed down, it&#39;s time to do a systematic grid search. This takes the cartesian products of the discrete list of options, i.e. every combination, and run trials. This is the most commonly used, and could help us properly see the heatmap when we probe the search space systematically.</p>
<h3 id='randomsearch'>RandomSearch</h3><pre class="highlight plaintext"><code>Random Search by sampling on hysphere around a search path:
1. init x a random position in space
2. until termination (max_eval or fitness, e.g. solved all), do:
    2.1 sample new pos some radius away: next_x = x + r
    2.2 if f(next_x) &gt; f(x) then set x = next_x
</code></pre>
<p>After GridSearch, there are usually intermediate values between the grid points that could yield slightly higher fitness score. Narrow down even further to the best search space as shown in the GridSearch heatmap, and do a random search. This is the final fine-tuning. The RandomSearch algorithm is directional and greedy.</p>
<h3 id='hyperoptimizer-roadmap'>HyperOptimizer Roadmap</h3>
<p>These are the future hyperparameter optimization algorithms we&#39;d like to implement standalone in the Lab. The implementations for them currently exists, but they&#39;re too bloated, and their engineering aspects are not ideal for the Lab.</p>

<ul>
<li><a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">TPE</a> / <a href="https://github.com/hyperopt/hyperopt">hyperopt</a></li>
<li><a href="https://github.com/HIPS/Spearmint">Bayesian Optimizer (Spearmint)</a></li>
<li><a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/#software">SMAC</a></li>
</ul>
<h3 id='implementation-guideline'>Implementation Guideline</h3>
<p>All implemented hyperoptimizers shall extend the base <code>HyperOptimizer</code> class in <code>rl/hyperoptimizer/base_hyperoptimizer.py</code> and follow its design for compatibility. Below we show this design to be general theoretically and practically. Moreover, do not use bloated dependencies.</p>

<p><strong>Theoretical design:</strong></p>

<p>A hyperoptimizer is a function <code>h</code> that takes:</p>

<ul>
<li>a trial (objective) function <code>Trial</code></li>
<li>a parameter space <code>P</code> (implemented in <code>experiment_spec</code>)</li>
</ul>

<p>and runs the algorithm:</p>

<ol>
<li>search the next <code>p</code> in <code>P</code> using its internal search algorithm, add to its internal <code>param_search_list</code>.</li>
<li>run a (slow) function <code>Trial(p) = fitness_score</code> (inside trial data)</li>
<li>update search using the feedback <code>fitness_score</code></li>
<li>repeat until max steps or fitness condition met</li>
</ol>

<p>Note that the search space <code>P</code> is a tensor space product of <code>m</code> bounded real spaces <code>R</code> and <code>n</code> bounded discrete spaces <code>N</code>. The search path in <code>param_search_list</code> must also be well-ordered to ensure resumability.</p>

<p><strong>Implementation requirements:</strong></p>

<ol>
<li>we want order-preserving and persistence in search for the ability to resume/reproduce an experiment.</li>
<li>the search algorithm may have its own internal memory/belief to facilitate search.</li>
<li>the <code>Trial</code> function shall be treated as a blackbox <code>Trial(p) = fitness_score</code> with input/output <code>(p, fitness_score)</code> for the generality of implementation/</li>
</ol>

<p><strong>Specification of search space:</strong></p>

<p>1. for real variable, specify a distribution (an interval is just a uniformly distributed space) in the <code>experiment_spec.param_range</code>. Example:</p>
<pre class="highlight json tab-json"><code><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"min"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0005</span><span class="p">,</span><span class="w">
  </span><span class="s2">"max"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.05</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>2. for discrete variable, specify a list of the values to search over (since it is finite anyway) in the <code>experiment_spec.param_range</code>. This will automatically be sorted when read into <code>HyperOptimizer</code> to ensure ordering. Example:</p>
<pre class="highlight json tab-json"><code><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2</span><span class="p">]</span><span class="w">
</span></code></pre>
<p>The hyperopt implementation shall be able to take these 2 types of param_range specs and construct its search space.</p>

<p>Note that whether a variable is real or discrete can be up to the user; some variable such as <code>lr</code> can be sampled from interval <code>0.001 to 0.1</code> or human-specified options <code>[0.01, 0.02, 0.05, 0.1, 0.2]</code>. One way may be more efficient than the other depending on the search algorithm.</p>

<p>The experiment will run it as:</p>
<pre class="highlight python tab-python"><code><span class="c"># specify which hyperoptimizer class to use in spec for bookkeeping</span>
<span class="n">Hopt</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">GREF</span><span class="p">,</span> <span class="n">experiment_spec</span><span class="p">[</span><span class="s">'HyperOptimizer'</span><span class="p">])</span>
<span class="n">hopt</span> <span class="o">=</span> <span class="n">Hopt</span><span class="p">(</span><span class="n">Trial</span><span class="p">,</span> <span class="o">**</span><span class="n">experiment_kwargs</span><span class="p">)</span>
<span class="n">experiment_data</span> <span class="o">=</span> <span class="n">hopt</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre><h2 id='preprocessor'>Preprocessor</h2>
<p><strong>code location:</strong> <code>rl/preprocessor/</code></p>

<p>Sometimes preprocessing the states before they are received by the agent can help to simplify the problem or make the agent strong. One example is the pixel preprocessing the removes color channels and rescales image size, in order to reduce unnecessary information overload. The other is to concat states from sequential timesteps to present richer, correlated information that is otherwise sparse.</p>

<p>The change in dimensions after preprocessing is handled automatically, so you can use them without any concerns.</p>
<h3 id='nopreprocessor'>NoPreProcessor</h3>
<p>The default that does not preprocess, but pass on the states as is.</p>
<h3 id='stackstates'>StackStates</h3>
<p>Concat the current and the previous states. Turns out this boosts agent performance in the <code>LunarLander-v2</code> environment.</p>
<h3 id='diffstates'>DiffStates</h3>
<p>Take the difference <code>new_states - old_states</code>.</p>
<h3 id='atari'>Atari</h3>
<p>Convert images to greyscale, downsize, crop, then stack 4 most recent states together. Useful for the Atari environments.</p>

          <h1 id='a-name-development-a-development'><a name="development"></a>Development</h1>
<p>For agent-specific development, see <a href="#agents">Agents</a>. This section details the general, non-agent development guideline.</p>

<p>The design of the code is clean enough to simply infer how things work by existing examples. The fastest way is to develop is to dig into the source code.</p>

<ul>
<li><code>data/</code>: data folders grouped per experiment, each of which contains all the graphs per trial sessions, JSON data file per trial, and csv metrics dataframe per run of multiple trials</li>
<li><code>rl/agent/</code>: custom agents. Refer to <code>base_agent.py</code> and <code>dqn.py</code> to build your own</li>
<li><code>rl/hyperoptimizer/</code>: Hyperparameter optimizers for the Experiments</li>
<li><code>rl/memory/</code>: RL agent memory classes</li>
<li><code>rl/optimizer/</code>: RL agent NN optimizer classes</li>
<li><code>rl/policy/</code>: RL agent policy classes</li>
<li><code>rl/preprocessor/</code>: RL agent preprocessor (state and memory) classes</li>
<li><code>rl/spec/</code>: specify new problems and experiment_specs to run experiments for</li>
<li><code>rl/spec/component_locks.json</code>: locks to check RL component combination in specs</li>
<li><code>rl/analytics.py</code>: the data analytics module for output experiment data</li>
<li><code>rl/experiment.py</code>: the main high level experiment logic</li>
<li><code>rl/util.py</code>: Generic util</li>
</ul>

<aside class="notice">
As the Lab grows, we will add more development guide as needed.
</aside>
<h2 id='a-name-roadmap-a-roadmap'><a name="roadmap"></a>Roadmap</h2>
<p>Check the latest under the <a href="https://github.com/kengz/openai_lab/projects">Github Projects</a></p>

          <h1 id='a-name-contributing-a-contributing'><a name="contributing"></a>Contributing</h1>
<ul>
<li>Invented a new algorithm/added some features? Submit a <a href="https://github.com/kengz/openai_lab/pulls">Pull Request here</a>.</li>
<li>Created some stronger agents that beats the current results? Read <a href="#solutions">here to submit new solution</a>.</li>
<li>Wanna just talk? Hit us up on Twitter below.</li>
</ul>
<h3 id='lab-contributors'>Lab Contributors</h3>
<p>See the <a href="https://github.com/kengz/openai_lab/graphs/contributors">full list of contributors here</a>.</p>
<h2 id='authors'>Authors</h2>
<p><em>Note: we are not affiliated with OpenAI; OpenAI Lab is not tied to any organizations.</em></p>

<ul>
<li>Wah Loon Keng. <a href="https://twitter.com/kengzwl">twitter: @kengzwl</a></li>
<li>Laura Graesser. <a href="https://twitter.com/lgraesser3">twitter: @lgraesser3</a></li>
</ul>

<p><em>OpenAI we hope you find this useful!</em></p>

          <h1 id='a-name-motivations-a-motivations'><a name="motivations"></a>Motivations</h1>
<p><em>This section is more casual, but we thought we&#39;d share the motivations behind the Lab.</em></p>

<p>We the authors never set out to build OpenAI Lab with any grand vision in mind. We just wanted to test our RL ideas in OpenAI Gym, faced many problems along the way, and their solutions became features. These opened up new adjacent possibles to do new things, and even more problems, and so on. Before we knew it, the critical components fell it place and we had something very similar to a scientific lab.</p>

<p>The problems faced by us are numerous and diverse, but there are several major categories. The first two are nicely described by WildML&#39;s Denny in his post <a href="http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/">Engineering Is The Bottleneck In (Deep Learning) Research</a>, which resonates strongly with a lot of people.</p>

<p><strong>1. the difficulty of building upon other&#39;s work</strong></p>

<p>If you have tried to implement any algorithms by looking at someone elses code, chances are it&#39;s painful. Sometimes you just want to research a small component like a prioritized memory, but you&#39;d have to write 90% of the unrelated components from scratch. Simply look at the solution source codes submitted to the OpenAI Gym leaderboard; you can&#39;t extend them to build something much bigger.</p>

<p>Of many implementations we saw which solve OpenAI gym environments, many had to rewrite the same basic components instead of just the new components being researched. This is unnecessary and inefficient.</p>

<p>There is no design or engineering standards for reinforcement learning, and that contributes to the major inertia in RL research. A lot of times research ideas are not difficult to come by, but implementing them is hard because there is <em>no reliable foundation to build on</em>.</p>

<p>We patiently built every piece of that foundation because giving up wasn&#39;t an option, so here it is. As the Lab grows, we hope that engineers and researchers can experiment with an idea fast by building on top of our existing components, and of course, contribute back.</p>

<p><strong>2. the lack of rigor in comparisons</strong></p>

<p>Denny describes this already, <a href="http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/">read his blog</a>.</p>

<p>As the Lab became mature, we became more ambitious and try to solve more environment, with more agents. This naturally begs the question, &quot;how do we compare them, across agents and environments?&quot;</p>

<p>Multiple experiments running in the Lab will produce standardized data analytics and evaluation metrics. This will allow us to compare agents and environments meaningfully, and that is the point of the Lab&#39;s <a href="#fitness-matrix">Fitness Matrix</a>. It also inspired a <a href="#metrics">generalization of evaluation metrics</a>, which we have only discovered recently.</p>

<p><strong>3. the inertia to high level vision</strong></p>

<p>When you&#39;re heels down implementing an algorithm and the extra 90% side components from scratch, it&#39;s hard to organize your work from a high level. Having to worry about other irrelevant components also makes you lose focus. The Lab removes that inertia and frees us from that myopic vision.</p>

<p>This freedom means more mental energy and time to focus on the essential components of research. It opens up new adjacent possibles and has us asking new questions.</p>

<p><strong>The New Adjacent Possibles</strong></p>

<p>With those problems above resolved, the Lab opens up the new adjacent possibles and allows us to do more. Below are some:</p>

<ul>
<li>independent discovery of RankedMemory, which is essentially the Priorized Experience Replay (PER) idea.</li>
<li>the study of hyperparameters at scale, with the <a href="#analysis">Analysis Graph</a>.</li>
<li>an explosion of experiments; suddenly we have the ability to test a lot of ideas quickly.</li>
<li>new high level, powerful visuals from the <a href="#analysis">analysis graphs</a> to eyeball the performance and potentials of an agent.</li>
<li>auto-architecture of agent&#39;s neural net, because we wanted to study some variables of the NN architecture as hyperparameters.</li>
<li>the perspective of RL as experimental science, and experimenting as breeding agents in the environments.</li>
<li>the design of a new <a href="#fitness"><code>fitness_score</code> that provides richer evaluation metrics</a>, which considers solution speed, stability, consistency, and much more.</li>
<li>the <a href="#fitness-matrix">Fitness Matrix</a> and the ability to compare fitness_score across its grid.</li>
<li>the <a href="#generalization">generalization of evaluation metrics</a>, which casts the hyperoptimization problem as one of temperature fields and differential calculus. Also gives the Fitness Fields Matrix.</li>
</ul>
<h2 id='a-name-who-should-use-a-who-should-use-openai-lab'><a name="who-should-use"></a>Who Should Use OpenAI Lab?</h2>
<p>We think this framework is useful for two types of users, those who are new to RL and RL researchers / advanced RL users.</p>
<h3 id='newcomers-to-rl'>Newcomers to RL</h3>
<p>For users that are new to RL, there is a lot to get your head around before you can get started. Understanding the Open AI gym environments and how to work with them, understanding RL algorithms, and understanding neural networks and their role as function approximators. OpenAI Lab reduces the inertia to begin.</p>

<p>We provide a range of implemented algorithms and components, as well as solutions to OpenAI gym environments that can be used out of the box. We also wanted to give new users the flexibility to change as much as possible in their experiments so we parameterized many of the algorithm variables and exposed them through a simple JSON interface.</p>

<p>This makes it possible for new users to spend their time experimenting with the different components and parameter settings, understanding what they do and how they affect an agent&#39;s performance, instead of going through the time consuming process of having to implement things themselves. </p>
<h3 id='rl-researchers-advanced-users'>RL Researchers / Advanced Users</h3>
<p>For advanced users or RL researchers, the Lab makes it easier to develop new ideas (a new way of sampling from memory for example) because a user needs to write only that code and can reuse the other components.</p>

<p>It also makes it easier to build on other peoples work. Solutions submitted via the lab&#39;s framework encapsulate all of the code and parameter settings, so that the result can be reproduced in just a few lines of code.</p>

<p>Additionally, the experiment framework and analytics allows better measurement of their ideas, and standardization provides meaningful comparisons among algorithms.</p>
<h2 id='a-name-main-contributions-a-main-contributions'><a name="main-contributions"></a>Main Contributions</h2>
<p>We see the main new contributions/features of the Lab as follows:</p>

<ol>
<li><strong>Unified RL environment and agent interface</strong> using OpenAI Gym, Tensorflow, Keras, so you can focus on developing the algorithms.</li>
<li><strong><a href="#agents-matrix">Implemented core RL algorithms</a>, with reusable modular components</strong> for developing deep RL algorithms.</li>
<li><strong>An experimentation framework</strong> for running hundreds of trials of hyperparameter optimizations, with logs, plots and analytics for testing new RL algorithms. Experimental settings are stored in standardized JSONs for reproducibility and comparisons.</li>
<li><strong>Automated analytics of the experiments</strong> for evaluating the RL agents and environments, and to help pick the best solution.</li>
<li><strong>The <a href="#fitness-matrix">Fitness matrix</a></strong>, a table of the best scores of RL algorithms v.s. the environments; useful for research.</li>
</ol>

<p>1 - 4 have been covered in the sections above. Here we&#39;ll focus on the <a href="#fitness-matrix">Fitness matrix</a></p>

<p>The Fitness Matrix compares algorithms and environments. Tables playing a similar role to this are often seen in RL research papers from DeepMind or OpenAI.</p>

<p>Clearly it would take a lot for an individual to produce something similar - first to implement so many algorithms, then to run them across many environments, which would consume a lot of computational time.</p>

<p>We think the Fitness Matrix could be useful for RL researchers or new users as a benchmark to evaluate their new algorithms. An open source fitness matrix means everyone can use it instead of building one from scratch.</p>

<p>We see this as an extension of OpenAI gym&#39;s Evaluation boards (<a href="https://gym.openai.com/envs/CartPole-v0">example: CartPole-v0</a>) in two ways:
- First, the score is richer; it takes into account not just the speed to solve a problem and the maximum score, but also the stability and consistency of solutions. See <a href="#fitness">Fitness Score</a>.
- Second, it makes comparison of environments and the comparison of agents across environments possible.</p>

<p>Our long term goal is to build a community of users and contributors around this framework. The more contributors, the more advanced the algorithms that are implemented and comparable wth the Lab. If successful, it could serve as the standardized benchmark for a large class of algorithms and a large class of problems. It becomes a public, living record which can be used by researchers who need to evaluate new algorithms and have no access to detailed benchmarks (unless if you&#39;re Google/OpenAI). </p>

<p>With these metrics across multiple environments and agents, we can characterize, say, if an agent is strong (solves many problems); or if a generic component like Prioritized Memory Replay can increase the score of agent across all environments.</p>

<p>We also think the Fitness matrix could be used to further research, e.g. let us study cross-environment evaluation metrics, find patterns, and classify the characteristic of problems and agents. For example, we noticed that some classes of problems require spacial reasoning, or multi-step learning to solve; and there are agents who could or could not do that.</p>

<p>For the formulation of these measurements and more details, see <a href="#generalized-metrics">generalized metrics</a>. Such high level, cross-environment / agent analysis seems fairly new, and could prove useful to researchers. We would like to continue our work on this.</p>

      </div>
      <div class="dark-box">
      </div>
    </div>
  </body>
</html>
